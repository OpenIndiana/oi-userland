diff --git a/js/src/Makefile.in b/js/src/Makefile.in
index fadd850..b719c30 100644
--- a/Makefile.in
+++ b/Makefile.in
@@ -350,6 +350,9 @@ endif
 ifeq (arm, $(TARGET_CPU))
 #CPPSRCS		+= only_on_arm.cpp
 endif
+ifeq (sparc, $(findstring sparc,$(TARGET_CPU)))
+ASFILES +=	TrampolineSparc.s
+endif
 #
 # END enclude sources for the method JIT
 #############################################
@@ -375,7 +378,7 @@ CPPSRCS += 	checks.cc \
 # END enclude sources for V8 dtoa
 #############################################
 
-ifeq (,$(filter-out powerpc sparc,$(TARGET_CPU)))
+ifeq (,$(filter-out powerpc,$(TARGET_CPU)))
 
 VPATH +=	$(srcdir)/assembler \
 		$(srcdir)/assembler/wtf \
diff --git a/js/src/assembler/assembler/AbstractMacroAssembler.h b/js/src/assembler/assembler/AbstractMacroAssembler.h
index 77958d9..d06a213 100644
--- a/assembler/assembler/AbstractMacroAssembler.h
+++ b/assembler/assembler/AbstractMacroAssembler.h
@@ -522,6 +522,11 @@ public:
         return AssemblerType::getDifferenceBetweenLabels(from.m_label, to.m_label);
     }
 
+    ptrdiff_t differenceBetween(DataLabel32 from, Jump to)
+    {
+        return AssemblerType::getDifferenceBetweenLabels(from.m_label, to.m_jmp);
+    }
+
     ptrdiff_t differenceBetween(DataLabelPtr from, Jump to)
     {
         return AssemblerType::getDifferenceBetweenLabels(from.m_label, to.m_jmp);
diff --git a/js/src/assembler/assembler/MacroAssembler.h b/js/src/assembler/assembler/MacroAssembler.h
index 5f24981..73bda22 100644
--- a/assembler/assembler/MacroAssembler.h
+++ b/assembler/assembler/MacroAssembler.h
@@ -54,6 +54,10 @@ namespace JSC { typedef MacroAssemblerX86 MacroAssemblerBase; }
 #include "MacroAssemblerX86_64.h"
 namespace JSC { typedef MacroAssemblerX86_64 MacroAssemblerBase; }
 
+#elif WTF_CPU_SPARC
+#include "MacroAssemblerSparc.h"
+namespace JSC { typedef MacroAssemblerSparc MacroAssemblerBase; }
+
 #else
 #error "The MacroAssembler is not supported on this platform."
 #endif
diff --git a/js/src/assembler/assembler/MacroAssemblerSparc.h b/js/src/assembler/assembler/MacroAssemblerSparc.h
new file mode 100644
index 0000000..a2a039c
--- /dev/null
+++ b/assembler/assembler/MacroAssemblerSparc.h
@@ -0,0 +1,1485 @@
+/* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
+ * vim: set ts=4 sw=4 et tw=99:
+ *
+ * ***** BEGIN LICENSE BLOCK *****
+ * Version: MPL 1.1/GPL 2.0/LGPL 2.1
+ *
+ * The contents of this file are subject to the Mozilla Public License Version
+ * 1.1 (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ * http://www.mozilla.org/MPL/
+ *
+ * Software distributed under the License is distributed on an "AS IS" basis,
+ * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
+ * for the specific language governing rights and limitations under the
+ * License.
+ *
+ * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
+ * May 28, 2008.
+ *
+ * The Initial Developer of the Original Code is
+ * Leon Sha <leon.sha@oracle.com>
+ * 
+ * Portions created by the Initial Developer are Copyright (C) 2010-2011
+ * the Initial Developer. All Rights Reserved.
+ *
+ * Contributor(s):
+ *
+ * Alternatively, the contents of this file may be used under the terms of
+ * either the GNU General Public License Version 2 or later (the "GPL"), or
+ * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
+ * in which case the provisions of the GPL or the LGPL are applicable instead
+ * of those above. If you wish to allow use of your version of this file only
+ * under the terms of either the GPL or the LGPL, and not to allow others to
+ * use your version of this file under the terms of the MPL, indicate your
+ * decision by deleting the provisions above and replace them with the notice
+ * and other provisions required by the GPL or the LGPL. If you do not delete
+ * the provisions above, a recipient may use your version of this file under
+ * the terms of any one of the MPL, the GPL or the LGPL.
+ *
+ * ***** END LICENSE BLOCK ***** */
+
+#ifndef MacroAssemblerSparc_h
+#define MacroAssemblerSparc_h
+
+#include <wtf/Platform.h>
+
+#if ENABLE_ASSEMBLER && WTF_CPU_SPARC
+
+#include "SparcAssembler.h"
+#include "AbstractMacroAssembler.h"
+
+namespace JSC {
+
+    class MacroAssemblerSparc : public AbstractMacroAssembler<SparcAssembler> {
+    public:
+        enum Condition {
+            Equal = SparcAssembler::ConditionE,
+            NotEqual = SparcAssembler::ConditionNE,
+            Above = SparcAssembler::ConditionGU,
+            AboveOrEqual = SparcAssembler::ConditionCC,
+            Below = SparcAssembler::ConditionCS,
+            BelowOrEqual = SparcAssembler::ConditionLEU,
+            GreaterThan = SparcAssembler::ConditionG,
+            GreaterThanOrEqual = SparcAssembler::ConditionGE,
+            LessThan = SparcAssembler::ConditionL,
+            LessThanOrEqual = SparcAssembler::ConditionLE,
+            Overflow = SparcAssembler::ConditionVS,
+            Signed = SparcAssembler::ConditionNEG,
+            Zero = SparcAssembler::ConditionE,
+            NonZero = SparcAssembler::ConditionNE
+        };
+
+        enum DoubleCondition {
+            // These conditions will only evaluate to true if the comparison is ordered - i.e. neither operand is NaN.
+            DoubleEqual = SparcAssembler::DoubleConditionE,
+            DoubleNotEqual = SparcAssembler::DoubleConditionNE,
+            DoubleGreaterThan = SparcAssembler::DoubleConditionG,
+            DoubleGreaterThanOrEqual = SparcAssembler::DoubleConditionGE,
+            DoubleLessThan = SparcAssembler::DoubleConditionL,
+            DoubleLessThanOrEqual = SparcAssembler::DoubleConditionLE,
+            // If either operand is NaN, these conditions always evaluate to true.
+            DoubleEqualOrUnordered = SparcAssembler::DoubleConditionUE,
+            DoubleNotEqualOrUnordered = SparcAssembler::DoubleConditionNE,
+            DoubleGreaterThanOrUnordered = SparcAssembler::DoubleConditionUG,
+            DoubleGreaterThanOrEqualOrUnordered = SparcAssembler::DoubleConditionUGE,
+            DoubleLessThanOrUnordered = SparcAssembler::DoubleConditionUL,
+            DoubleLessThanOrEqualOrUnordered = SparcAssembler::DoubleConditionULE
+        };
+
+        static const RegisterID stackPointerRegister = SparcRegisters::sp;
+
+        static const Scale ScalePtr = TimesFour;
+        static const unsigned int TotalRegisters = 32;
+
+        void add32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.addcc_r(dest, src, dest);
+        }
+
+        void add32(Imm32 imm, Address address)
+        {
+            load32(address, SparcRegisters::g2);
+            add32(imm, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address);
+        }
+
+        void add32(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.addcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.addcc_r(dest, SparcRegisters::g3, dest);
+            }
+        }
+
+        void add32(Address src, RegisterID dest)
+        {
+            load32(src, SparcRegisters::g2);
+            m_assembler.addcc_r(dest, SparcRegisters::g2, dest);
+        }
+
+        void and32(Address src, RegisterID dest)
+        {
+            load32(src, SparcRegisters::g2);
+            m_assembler.andcc_r(dest, SparcRegisters::g2, dest);
+        }
+
+        void add32(Imm32 imm, RegisterID src, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.addcc_imm(src, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.addcc_r(src, SparcRegisters::g3, dest);
+            }
+        }
+
+        void and32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.andcc_r(dest, src, dest);
+        }
+
+        void and32(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.andcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.andcc_r(dest, SparcRegisters::g3, dest);
+            }
+        }
+
+
+        void lshift32(RegisterID shift_amount, RegisterID dest)
+        {
+            m_assembler.sll_r(dest, shift_amount, dest);
+        }
+
+        void lshift32(Imm32 imm, RegisterID dest)
+        {
+            // No need to check if imm.m_value.
+            // The last 5 bit of imm.m_value will be used anyway.
+            m_assembler.sll_imm(dest, imm.m_value, dest);
+        }
+
+        void mul32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.smulcc_r(dest, src, dest);
+        }
+
+        void mul32(Imm32 imm, RegisterID src, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.smulcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.smulcc_r(SparcRegisters::g3, dest, dest);
+            }
+        }
+
+        void neg32(RegisterID srcDest)
+        {
+            m_assembler.subcc_r(SparcRegisters::g0, srcDest, srcDest);
+        }
+
+        void not32(RegisterID dest)
+        {
+            m_assembler.xnorcc_r(dest, SparcRegisters::g0, dest);
+        }
+
+        void or32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.orcc_r(dest, src, dest);
+        }
+
+        void or32(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.orcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.or_r(SparcRegisters::g3, dest, dest);
+            }
+        }
+
+
+        void or32(Address address, RegisterID dest)
+        {
+            load32(address, SparcRegisters::g2);
+            or32(SparcRegisters::g2, dest);
+        }
+
+        void rshift32(RegisterID shift_amount, RegisterID dest)
+        {
+            m_assembler.sra_r(dest, shift_amount, dest);
+        }
+
+        void rshift32(Imm32 imm, RegisterID dest)
+        {
+            // No need to check if imm.m_value.
+            // The last 5 bit of imm.m_value will be used anyway.
+            m_assembler.sra_imm(dest, imm.m_value, dest);
+        }
+    
+        void urshift32(RegisterID shift_amount, RegisterID dest)
+        {
+            m_assembler.srl_r(dest, shift_amount, dest);
+        }
+    
+        void urshift32(Imm32 imm, RegisterID dest)
+        {
+            // No need to check if imm.m_value.
+            // The last 5 bit of imm.m_value will be used anyway.
+            m_assembler.srl_imm(dest, imm.m_value, dest);
+        }
+
+        void sub32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.subcc_r(dest, src, dest);
+        }
+
+        void sub32(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.subcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.subcc_r(dest, SparcRegisters::g3, dest);
+            }
+        }
+
+        void sub32(Imm32 imm, Address address)
+        {
+            load32(address, SparcRegisters::g2);
+            sub32(imm, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address);
+        }
+
+        void sub32(Address src, RegisterID dest)
+        {
+            load32(src, SparcRegisters::g2);
+            sub32(SparcRegisters::g2, dest);
+        }
+
+        void xor32(RegisterID src, RegisterID dest)
+        {
+            m_assembler.xorcc_r(src, dest, dest);
+        }
+
+        void xor32(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.xorcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.xorcc_r(dest, SparcRegisters::g3, dest);
+            }
+        }
+
+        void xor32(Address src, RegisterID dest)
+        {
+            load32(src, SparcRegisters::g2);
+            xor32(SparcRegisters::g2, dest);
+        }
+
+        void load8(ImplicitAddress address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldub_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldub_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void load32(ImplicitAddress address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.lduw_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.lduw_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void load32(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.lduw_r(address.base, SparcRegisters::g2, dest);
+        }
+
+        void load32WithUnalignedHalfWords(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset+3), SparcRegisters::g2);
+            m_assembler.ldub_r(address.base, SparcRegisters::g2, dest);
+            m_assembler.subcc_imm(SparcRegisters::g2, 1, SparcRegisters::g2);
+            m_assembler.ldub_r(address.base, SparcRegisters::g2, SparcRegisters::g3);
+            m_assembler.sll_imm(SparcRegisters::g3, 8, SparcRegisters::g3);
+            m_assembler.or_r(SparcRegisters::g3, dest, dest);
+            m_assembler.subcc_imm(SparcRegisters::g2, 1, SparcRegisters::g2);
+            m_assembler.ldub_r(address.base, SparcRegisters::g2, SparcRegisters::g3);
+            m_assembler.sll_imm(SparcRegisters::g3, 16, SparcRegisters::g3);
+            m_assembler.or_r(SparcRegisters::g3, dest, dest);
+            m_assembler.subcc_imm(SparcRegisters::g2, 1, SparcRegisters::g2);
+            m_assembler.ldub_r(address.base, SparcRegisters::g2, SparcRegisters::g3);
+            m_assembler.sll_imm(SparcRegisters::g3, 24, SparcRegisters::g3);
+            m_assembler.or_r(SparcRegisters::g3, dest, dest);
+        }
+
+        DataLabel32 load32WithAddressOffsetPatch(Address address, RegisterID dest)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(0, SparcRegisters::g3);
+            m_assembler.lduw_r(address.base, SparcRegisters::g3, dest);
+            return dataLabel;
+        }
+
+        DataLabel32 load64WithAddressOffsetPatch(Address address, RegisterID hi, RegisterID lo)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(0, SparcRegisters::g3);
+            m_assembler.add_imm(SparcRegisters::g3, 4, SparcRegisters::g2);
+            m_assembler.lduw_r(address.base, SparcRegisters::g3, hi);
+            m_assembler.lduw_r(address.base, SparcRegisters::g2, lo);
+            return dataLabel;
+        }
+
+        Label loadPtrWithPatchToLEA(Address address, RegisterID dest)
+        {
+            Label label(this);
+            load32(address, dest);
+            return label;
+        }
+
+        void load16(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.lduh_r(address.base, SparcRegisters::g2, dest);
+        }
+    
+        void load16(ImplicitAddress address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.lduh_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.lduh_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void store8(RegisterID src, ImplicitAddress address)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.stb_imm(src, address.base, address.offset);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.stb_r(src, address.base, SparcRegisters::g3);
+            }
+        }
+
+        void store8(RegisterID src, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.stb_r(src, address.base, SparcRegisters::g2);
+        }
+
+        void store8(Imm32 imm, ImplicitAddress address)
+        {
+            m_assembler.move_nocheck(imm.m_value, SparcRegisters::g2);
+            store8(SparcRegisters::g2, address);
+        }
+
+        void store8(Imm32 imm, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            move(imm, SparcRegisters::g3);
+            m_assembler.stb_r(SparcRegisters::g3, SparcRegisters::g2, address.base);
+        }
+
+        void store16(RegisterID src, ImplicitAddress address)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.sth_imm(src, address.base, address.offset);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.sth_r(src, address.base, SparcRegisters::g3);
+            }
+        }
+
+        void store16(RegisterID src, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.sth_r(src, address.base, SparcRegisters::g2);
+        }
+
+        void store16(Imm32 imm, ImplicitAddress address)
+        {
+            m_assembler.move_nocheck(imm.m_value, SparcRegisters::g2);
+            store16(SparcRegisters::g2, address);
+        }
+
+        void store16(Imm32 imm, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            move(imm, SparcRegisters::g3);
+            m_assembler.sth_r(SparcRegisters::g3, SparcRegisters::g2, address.base);
+        }
+
+        void load8ZeroExtend(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.ldub_r(address.base, SparcRegisters::g2, dest);
+        }
+
+        void load8ZeroExtend(Address address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldub_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldub_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void load8SignExtend(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.ldsb_r(address.base, SparcRegisters::g2, dest);
+        }
+
+        void load8SignExtend(Address address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldsb_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldsb_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void load16SignExtend(BaseIndex address, RegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.ldsh_r(address.base, SparcRegisters::g2, dest);
+        }
+
+        void load16SignExtend(Address address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldsh_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldsh_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        DataLabel32 store32WithAddressOffsetPatch(RegisterID src, Address address)
+        {
+            DataLabel32 dataLabel(this);
+            // Since this is for patch, we don't check is offset is imm13.
+            m_assembler.move_nocheck(0, SparcRegisters::g3);
+            m_assembler.stw_r(src, address.base, SparcRegisters::g3);
+            return dataLabel;
+        }
+
+
+        DataLabel32 store64WithAddressOffsetPatch(RegisterID hi, RegisterID lo, Address address)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+            m_assembler.add_r(SparcRegisters::g3, address.base, SparcRegisters::g3);
+            m_assembler.stw_imm(lo, SparcRegisters::g3, 4);
+            m_assembler.stw_imm(hi, SparcRegisters::g3, 0);
+            return dataLabel;
+        }
+
+        DataLabel32 store64WithAddressOffsetPatch(Imm32 hi, RegisterID lo, Address address)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+            m_assembler.add_r(SparcRegisters::g3, address.base, SparcRegisters::g3);
+            m_assembler.stw_imm(lo, SparcRegisters::g3, 4);
+            move(hi, SparcRegisters::g2);
+            m_assembler.stw_imm(SparcRegisters::g2, SparcRegisters::g3, 0);
+
+            return dataLabel;
+        }
+
+        DataLabel32 store64WithAddressOffsetPatch(Imm32 hi, Imm32 lo, Address address)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+            m_assembler.add_r(SparcRegisters::g3, address.base, SparcRegisters::g3);
+            move(lo, SparcRegisters::g2);
+            m_assembler.stw_imm(SparcRegisters::g2, SparcRegisters::g3, 4);
+            move(hi, SparcRegisters::g2);
+            m_assembler.stw_imm(SparcRegisters::g2, SparcRegisters::g3, 0);
+
+            return dataLabel;
+        }
+
+
+        void store32(RegisterID src, ImplicitAddress address)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.stw_imm(src, address.base, address.offset);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.stw_r(src, address.base, SparcRegisters::g3);
+            }
+        }
+
+        void store32(RegisterID src, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.stw_r(src, address.base, SparcRegisters::g2);
+        }
+
+        void store32(Imm32 imm, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            move(imm, SparcRegisters::g3);
+            m_assembler.stw_r(SparcRegisters::g3, SparcRegisters::g2, address.base);
+        }
+
+        void store32(Imm32 imm, ImplicitAddress address)
+        {
+            m_assembler.move_nocheck(imm.m_value, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address);
+        }
+
+        void store32(RegisterID src, void* address)
+        {
+            m_assembler.move_nocheck((int)address, SparcRegisters::g3);
+            m_assembler.stw_r(src, SparcRegisters::g0, SparcRegisters::g3);
+        }
+
+        void store32(Imm32 imm, void* address)
+        {
+            move(imm, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address);
+        }
+
+        void pop(RegisterID dest)
+        {
+            m_assembler.lduw_imm(SparcRegisters::sp, 0x68, dest);
+            m_assembler.addcc_imm(SparcRegisters::sp, 4, SparcRegisters::sp);
+        }
+
+        void push(RegisterID src)
+        {
+            m_assembler.subcc_imm(SparcRegisters::sp, 4, SparcRegisters::sp);
+            m_assembler.stw_imm(src, SparcRegisters::sp, 0x68);
+        }
+
+        void push(Address address)
+        {
+            load32(address, SparcRegisters::g2);
+            push(SparcRegisters::g2);
+        }
+
+        void push(Imm32 imm)
+        {
+            move(imm, SparcRegisters::g2);
+            push(SparcRegisters::g2);
+        }
+
+        void move(Imm32 imm, RegisterID dest)
+        {
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.or_imm(SparcRegisters::g0, imm.m_value, dest);
+            else
+                m_assembler.move_nocheck(imm.m_value, dest);
+        }
+
+        void move(RegisterID src, RegisterID dest)
+        {
+            m_assembler.or_r(src, SparcRegisters::g0, dest);
+        }
+
+        void move(ImmPtr imm, RegisterID dest)
+        {
+            move(Imm32(imm), dest);
+        }
+
+        void swap(RegisterID reg1, RegisterID reg2)
+        {
+            m_assembler.or_r(reg1, SparcRegisters::g0, SparcRegisters::g3);
+            m_assembler.or_r(reg2, SparcRegisters::g0, reg1);
+            m_assembler.or_r(SparcRegisters::g3, SparcRegisters::g0, reg2);
+        }
+
+        void signExtend32ToPtr(RegisterID src, RegisterID dest)
+        {
+            if (src != dest)
+                move(src, dest);
+        }
+
+        void zeroExtend32ToPtr(RegisterID src, RegisterID dest)
+        {
+            if (src != dest)
+                move(src, dest);
+        }
+
+        Jump branch8(Condition cond, Address left, Imm32 right)
+        {
+            load8(left, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Jump branch32_force32(Condition cond, RegisterID left, Imm32 right)
+        {
+            m_assembler.move_nocheck(right.m_value, SparcRegisters::g3);
+            m_assembler.subcc_r(left, SparcRegisters::g3, SparcRegisters::g0);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branch32FixedLength(Condition cond, RegisterID left, Imm32 right)
+        {
+            m_assembler.move_nocheck(right.m_value, SparcRegisters::g2);
+            return branch32(cond, left, SparcRegisters::g2);
+        }
+
+        Jump branch32WithPatch(Condition cond, RegisterID left, Imm32 right, DataLabel32 &dataLabel)
+        {
+            // Always use move_nocheck, since the value is to be patched.
+            dataLabel = DataLabel32(this);
+            m_assembler.move_nocheck(right.m_value, SparcRegisters::g3);
+            m_assembler.subcc_r(left, SparcRegisters::g3, SparcRegisters::g0);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branch32(Condition cond, RegisterID left, RegisterID right)
+        {
+            m_assembler.subcc_r(left, right, SparcRegisters::g0);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branch32(Condition cond, RegisterID left, Imm32 right)
+        {
+            if (m_assembler.isimm13(right.m_value))
+                m_assembler.subcc_imm(left, right.m_value, SparcRegisters::g0);
+            else {
+                m_assembler.move_nocheck(right.m_value, SparcRegisters::g3);
+                m_assembler.subcc_r(left, SparcRegisters::g3, SparcRegisters::g0);
+            }
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branch32(Condition cond, RegisterID left, Address right)
+        {
+            load32(right, SparcRegisters::g2);
+            return branch32(cond, left, SparcRegisters::g2);
+        }
+
+        Jump branch32(Condition cond, Address left, RegisterID right)
+        {
+            load32(left, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Jump branch32(Condition cond, Address left, Imm32 right)
+        {
+            load32(left, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Jump branch32(Condition cond, BaseIndex left, Imm32 right)
+        {
+
+            load32(left, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Jump branch32WithUnalignedHalfWords(Condition cond, BaseIndex left, Imm32 right)
+        {
+            load32WithUnalignedHalfWords(left, SparcRegisters::g4);
+            return branch32(cond, SparcRegisters::g4, right);
+        }
+
+        Jump branch16(Condition cond, BaseIndex left, RegisterID right)
+        {
+            (void)(cond);
+            (void)(left);
+            (void)(right);
+            ASSERT_NOT_REACHED();
+            return jump();
+        }
+
+        Jump branch16(Condition cond, BaseIndex left, Imm32 right)
+        {
+            load16(left, SparcRegisters::g3);
+            move(right, SparcRegisters::g2);
+            m_assembler.subcc_r(SparcRegisters::g3, SparcRegisters::g2, SparcRegisters::g0);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchTest8(Condition cond, Address address, Imm32 mask = Imm32(-1))
+        {
+            load8(address, SparcRegisters::g2);
+            return branchTest32(cond, SparcRegisters::g2, mask);
+        }
+
+        Jump branchTest32(Condition cond, RegisterID reg, RegisterID mask)
+        {
+            m_assembler.andcc_r(reg, mask, SparcRegisters::g0);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchTest32(Condition cond, RegisterID reg, Imm32 mask = Imm32(-1))
+        {
+            if (m_assembler.isimm13(mask.m_value))
+                m_assembler.andcc_imm(reg, mask.m_value, SparcRegisters::g0);
+            else {
+                m_assembler.move_nocheck(mask.m_value, SparcRegisters::g3);
+                m_assembler.andcc_r(reg, SparcRegisters::g3, SparcRegisters::g0);
+            }
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchTest32(Condition cond, Address address, Imm32 mask = Imm32(-1))
+        {
+            load32(address, SparcRegisters::g2);
+            return branchTest32(cond, SparcRegisters::g2, mask);
+        }
+
+        Jump branchTest32(Condition cond, BaseIndex address, Imm32 mask = Imm32(-1))
+        {
+            // FIXME. branchTest32 only used by PolyIC.
+            // PolyIC is not enabled for sparc now.
+            ASSERT(0);
+            return jump();
+        }
+
+        Jump jump()
+        {
+            return Jump(m_assembler.jmp());
+        }
+
+        void jump(RegisterID target)
+        {
+            m_assembler.jmpl_r(SparcRegisters::g0, target, SparcRegisters::g0);
+            m_assembler.nop();
+        }
+
+        void jump(Address address)
+        {
+            load32(address, SparcRegisters::g2);
+            m_assembler.jmpl_r(SparcRegisters::g2, SparcRegisters::g0, SparcRegisters::g0);
+            m_assembler.nop();
+        }
+
+        void jump(BaseIndex address)
+        {
+            load32(address, SparcRegisters::g2);
+            m_assembler.jmpl_r(SparcRegisters::g2, SparcRegisters::g0, SparcRegisters::g0);
+            m_assembler.nop();
+        }
+
+        Jump branchAdd32(Condition cond, RegisterID src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            m_assembler.addcc_r(src, dest, dest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchAdd32(Condition cond, Imm32 imm, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.addcc_imm(dest, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.addcc_r(dest, SparcRegisters::g3, dest);
+            }
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchAdd32(Condition cond, Address src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            load32(src, SparcRegisters::g2);
+            return branchAdd32(cond, SparcRegisters::g2, dest);
+        }
+
+        void mull32(RegisterID src1, RegisterID src2, RegisterID dest)
+        {
+            m_assembler.smulcc_r(src1, src2, dest);
+        }
+
+        Jump branchMul32(Condition cond, RegisterID src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            m_assembler.smulcc_r(src, dest, dest);
+            if (cond == Overflow) {
+                m_assembler.rdy(SparcRegisters::g2);
+                m_assembler.sra_imm(dest, 31, SparcRegisters::g3);
+                m_assembler.subcc_r(SparcRegisters::g2, SparcRegisters::g3, SparcRegisters::g2);
+                cond = NotEqual;
+            }
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchMul32(Condition cond, Imm32 imm, RegisterID src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            if (m_assembler.isimm13(imm.m_value))
+                m_assembler.smulcc_imm(src, imm.m_value, dest);
+            else {
+                m_assembler.move_nocheck(imm.m_value, SparcRegisters::g3);
+                m_assembler.smulcc_r(src, SparcRegisters::g3, dest);
+            }
+            if (cond == Overflow) {
+                m_assembler.rdy(SparcRegisters::g2);
+                m_assembler.sra_imm(dest, 31, SparcRegisters::g3);
+                m_assembler.subcc_r(SparcRegisters::g2, SparcRegisters::g3, SparcRegisters::g2);
+                cond = NotEqual;
+            }
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchMul32(Condition cond, Address src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            load32(src, SparcRegisters::g2);
+            return branchMul32(cond, SparcRegisters::g2, dest);
+        }
+
+        Jump branchSub32(Condition cond, RegisterID src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            m_assembler.subcc_r(dest, src, dest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchSub32(Condition cond, Imm32 imm, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            sub32(imm, dest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchSub32(Condition cond, Address src, RegisterID dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            load32(src, SparcRegisters::g2);
+            return branchSub32(cond, SparcRegisters::g2, dest);
+        }
+
+        Jump branchSub32(Condition cond, Imm32 imm, Address dest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            sub32(imm, dest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchNeg32(Condition cond, RegisterID srcDest)
+        {
+            ASSERT((cond == Overflow) || (cond == Signed) || (cond == Zero) || (cond == NonZero));
+            neg32(srcDest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        Jump branchOr32(Condition cond, RegisterID src, RegisterID dest)
+        {
+            ASSERT((cond == Signed) || (cond == Zero) || (cond == NonZero));
+            m_assembler.orcc_r(src, dest, dest);
+            return Jump(m_assembler.branch(SparcCondition(cond)));
+        }
+
+        void breakpoint()
+        {
+            m_assembler.ta_imm(8);
+        }
+
+        Call nearCall()
+        {
+            return Call(m_assembler.call(), Call::LinkableNear);
+        }
+
+        Call call(RegisterID target)
+        {
+            m_assembler.jmpl_r(target, SparcRegisters::g0, SparcRegisters::o7);
+            m_assembler.nop();
+            JmpSrc jmpSrc;
+            return Call(jmpSrc, Call::None);
+        }
+
+        void call(Address address)
+        {
+            if (m_assembler.isimm13(address.offset)) {
+                m_assembler.jmpl_imm(address.base, address.offset, SparcRegisters::o7);
+                m_assembler.nop();
+            } else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.jmpl_r(address.base, SparcRegisters::g3, SparcRegisters::o7);
+                m_assembler.nop();
+            }
+        }
+
+        void ret()
+        {
+            m_assembler.jmpl_imm(SparcRegisters::i7, 8, SparcRegisters::g0);
+            m_assembler.nop();
+        }
+
+        void ret_and_restore()
+        {
+            m_assembler.jmpl_imm(SparcRegisters::i7, 8, SparcRegisters::g0);
+            m_assembler.restore_r(SparcRegisters::g0, SparcRegisters::g0, SparcRegisters::g0);
+        }
+
+        void save(Imm32 size)
+        {
+            if (m_assembler.isimm13(size.m_value)) {
+                m_assembler.save_imm(SparcRegisters::sp, size.m_value, SparcRegisters::sp);
+            } else {
+                m_assembler.move_nocheck(size.m_value, SparcRegisters::g3);
+                m_assembler.save_r(SparcRegisters::sp, SparcRegisters::g3, SparcRegisters::sp);
+            }
+        }
+
+        void set32(Condition cond, Address left, RegisterID right, RegisterID dest)
+        {
+            load32(left, SparcRegisters::g2);
+            set32(cond, SparcRegisters::g2, right, dest);
+        }
+
+        void set32(Condition cond, RegisterID left, Address right, RegisterID dest)
+        {
+            load32(right, SparcRegisters::g2);
+            set32(cond, left, SparcRegisters::g2, dest);
+        }
+
+        void set32(Condition cond, RegisterID left, RegisterID right, RegisterID dest)
+        {
+            m_assembler.subcc_r(left, right, SparcRegisters::g0);
+            m_assembler.or_imm(SparcRegisters::g0, 0, dest);
+            m_assembler.movcc_imm(1, dest, SparcCondition(cond));
+        }
+
+        void set32(Condition cond, RegisterID left, Imm32 right, RegisterID dest)
+        {
+            if (m_assembler.isimm13(right.m_value))
+                m_assembler.subcc_imm(left, right.m_value, SparcRegisters::g0);
+            else {
+                m_assembler.move_nocheck(right.m_value, SparcRegisters::g3);
+                m_assembler.subcc_r(left, SparcRegisters::g3, SparcRegisters::g0);
+            }
+            m_assembler.or_imm(SparcRegisters::g0, 0, dest);
+            m_assembler.movcc_imm(1, dest, SparcCondition(cond));
+        }
+
+        void set32(Condition cond, Address left, Imm32 right, RegisterID dest)
+        {
+            load32(left, SparcRegisters::g2);
+            set32(cond, SparcRegisters::g2, right, dest);
+        }
+
+        void set8(Condition cond, RegisterID left, RegisterID right, RegisterID dest)
+        {
+            // Sparc does not have byte register.
+            set32(cond, left, right, dest);
+        }
+
+        void set8(Condition cond, Address left, RegisterID right, RegisterID dest)
+        {
+            // Sparc doesn't have byte registers
+            load32(left, SparcRegisters::g2);
+            set32(cond, SparcRegisters::g2, right, dest);
+        }
+
+        void set8(Condition cond, RegisterID left, Imm32 right, RegisterID dest)
+        {
+            // Sparc does not have byte register.
+            set32(cond, left, right, dest);
+        }
+
+        void setTest32(Condition cond, Address address, Imm32 mask, RegisterID dest)
+        {
+            load32(address, SparcRegisters::g2);
+            if (m_assembler.isimm13(mask.m_value))
+                m_assembler.andcc_imm(SparcRegisters::g2, mask.m_value, SparcRegisters::g0);
+            else {
+                m_assembler.move_nocheck(mask.m_value, SparcRegisters::g3);
+                m_assembler.andcc_r(SparcRegisters::g3, SparcRegisters::g2, SparcRegisters::g0);
+            }
+            m_assembler.or_imm(SparcRegisters::g0, 0, dest);
+            m_assembler.movcc_imm(1, dest, SparcCondition(cond));
+        }
+
+        void setTest8(Condition cond, Address address, Imm32 mask, RegisterID dest)
+        {
+            // Sparc does not have byte register.
+            setTest32(cond, address, mask, dest);
+        }
+
+        void lea(Address address, RegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.add_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.add_r(address.base, SparcRegisters::g3, dest);
+            }
+        }
+
+        void lea(BaseIndex address, RegisterID dest)
+        {
+            // lea only used by PolyIC.
+            // PolyIC is not enabled for sparc now.
+            ASSERT(0);
+        }
+
+        void add32(Imm32 imm, AbsoluteAddress address)
+        {
+            load32(address.m_ptr, SparcRegisters::g2);
+            add32(imm, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address.m_ptr);
+        }
+
+        void sub32(Imm32 imm, AbsoluteAddress address)
+        {
+            load32(address.m_ptr, SparcRegisters::g2);
+            sub32(imm, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address.m_ptr);
+        }
+
+        void load32(void* address, RegisterID dest)
+        {
+            m_assembler.move_nocheck((int)address, SparcRegisters::g3);
+            m_assembler.lduw_r(SparcRegisters::g3, SparcRegisters::g0, dest);
+        }
+
+        Jump branch32(Condition cond, AbsoluteAddress left, RegisterID right)
+        {
+            load32(left.m_ptr, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Jump branch32(Condition cond, AbsoluteAddress left, Imm32 right)
+        {
+            load32(left.m_ptr, SparcRegisters::g2);
+            return branch32(cond, SparcRegisters::g2, right);
+        }
+
+        Call call()
+        {
+            m_assembler.rdpc(SparcRegisters::g2);
+            m_assembler.add_imm(SparcRegisters::g2, 32, SparcRegisters::g2);
+            m_assembler.stw_imm(SparcRegisters::g2, SparcRegisters::fp, -8);
+            Call cl = Call(m_assembler.call(), Call::Linkable);
+            m_assembler.lduw_imm(SparcRegisters::fp, -8, SparcRegisters::g2);
+            m_assembler.jmpl_imm(SparcRegisters::g2, 0, SparcRegisters::g0);
+            m_assembler.nop();
+            return cl;
+        }
+
+        Call tailRecursiveCall()
+        {
+            return Call::fromTailJump(jump());
+        }
+
+        Call makeTailRecursiveCall(Jump oldJump)
+        {
+            return Call::fromTailJump(oldJump);
+        }
+
+        DataLabelPtr moveWithPatch(ImmPtr initialValue, RegisterID dest)
+        {
+            DataLabelPtr dataLabel(this);
+            Imm32 imm = Imm32(initialValue);
+            m_assembler.move_nocheck(imm.m_value, dest);
+            return dataLabel;
+        }
+
+        DataLabel32 moveWithPatch(Imm32 initialValue, RegisterID dest)
+        {
+            DataLabel32 dataLabel(this);
+            m_assembler.move_nocheck(initialValue.m_value, dest);
+            return dataLabel;
+        }
+
+        Jump branchPtrWithPatch(Condition cond, RegisterID left, DataLabelPtr& dataLabel, ImmPtr initialRightValue = ImmPtr(0))
+        {
+            dataLabel = moveWithPatch(initialRightValue, SparcRegisters::g2);
+            Jump jump = branch32(cond, left, SparcRegisters::g2);
+            return jump;
+        }
+
+        Jump branchPtrWithPatch(Condition cond, Address left, DataLabelPtr& dataLabel, ImmPtr initialRightValue = ImmPtr(0))
+        {
+            load32(left, SparcRegisters::g2);
+            dataLabel = moveWithPatch(initialRightValue, SparcRegisters::g3);
+            Jump jump = branch32(cond, SparcRegisters::g3, SparcRegisters::g2);
+            return jump;
+        }
+
+        DataLabelPtr storePtrWithPatch(ImmPtr initialValue, ImplicitAddress address)
+        {
+            DataLabelPtr dataLabel = moveWithPatch(initialValue, SparcRegisters::g2);
+            store32(SparcRegisters::g2, address);
+            return dataLabel;
+        }
+
+        DataLabelPtr storePtrWithPatch(ImplicitAddress address)
+        {
+            return storePtrWithPatch(ImmPtr(0), address);
+        }
+
+        // Floating point operators
+        bool supportsFloatingPoint() const
+        {
+            return true;
+        }
+
+        bool supportsFloatingPointTruncate() const
+        {
+            return true;
+        }
+
+        bool supportsFloatingPointSqrt() const
+        {
+            return true;
+        }
+
+        void moveDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fmovd_r(src, dest);
+        }
+
+        void loadFloat(BaseIndex address, FPRegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.ldf_r(address.base, SparcRegisters::g2, dest);
+            m_assembler.fstod_r(dest, dest);
+        }
+
+        void loadFloat(ImplicitAddress address, FPRegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldf_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldf_r(address.base, SparcRegisters::g3, dest);
+            }
+            m_assembler.fstod_r(dest, dest);
+        }
+
+        void loadFloat(const void* address, FPRegisterID dest)
+        {
+            m_assembler.move_nocheck((int)address, SparcRegisters::g3);
+            m_assembler.ldf_r(SparcRegisters::g3, SparcRegisters::g0, dest);
+            m_assembler.fstod_r(dest, dest);
+        }
+
+        void loadDouble(BaseIndex address, FPRegisterID dest)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.ldf_r(address.base, SparcRegisters::g2, dest);
+            m_assembler.add_imm(SparcRegisters::g2, 4, SparcRegisters::g2);
+            m_assembler.ldf_r(address.base, SparcRegisters::g2, dest + 1);
+        }
+
+        void loadDouble(ImplicitAddress address, FPRegisterID dest)
+        {
+            m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+            m_assembler.ldf_r(address.base, SparcRegisters::g3, dest);
+            m_assembler.add_imm(SparcRegisters::g3, 4, SparcRegisters::g3);
+            m_assembler.ldf_r(address.base, SparcRegisters::g3, dest + 1);
+        }
+
+        DataLabelPtr loadDouble(const void* address, FPRegisterID dest)
+        {
+            DataLabelPtr dataLabel(this);
+            m_assembler.move_nocheck((int)address, SparcRegisters::g3);
+            m_assembler.ldf_imm(SparcRegisters::g3, 0, dest);
+            m_assembler.ldf_imm(SparcRegisters::g3, 4, dest + 1);
+            return dataLabel;
+        }
+
+        void storeFloat(FPRegisterID src, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.stf_r(src, address.base, SparcRegisters::g2);
+        }
+
+        void storeFloat(FPRegisterID src, ImplicitAddress address)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.stf_imm(src, address.base, address.offset);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.stf_r(src, address.base, SparcRegisters::g3);
+            }
+        }
+
+        void storeFloat(ImmDouble imm, Address address)
+        {
+            union {
+                float f;
+                uint32 u32;
+            } u;
+            u.f = imm.u.d;
+            store32(Imm32(u.u32), address);
+        }
+
+        void storeFloat(ImmDouble imm, BaseIndex address)
+        {
+            union {
+                float f;
+                uint32 u32;
+            } u;
+            u.f = imm.u.d;
+            store32(Imm32(u.u32), address);
+        }
+
+        void storeDouble(FPRegisterID src, BaseIndex address)
+        {
+            m_assembler.sll_imm(address.index, address.scale, SparcRegisters::g2);
+            add32(Imm32(address.offset), SparcRegisters::g2);
+            m_assembler.stf_r(src, address.base, SparcRegisters::g2);
+            m_assembler.add_imm(SparcRegisters::g2, 4, SparcRegisters::g2);
+            m_assembler.stf_r(src + 1, address.base, SparcRegisters::g2);
+        }
+
+        void storeDouble(FPRegisterID src, ImplicitAddress address)
+        {
+            if (m_assembler.isimm13(address.offset + 4)) {
+                m_assembler.stf_imm(src, address.base, address.offset);
+                m_assembler.stf_imm(src + 1, address.base, address.offset + 4);
+            } else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.stf_r(src, address.base, SparcRegisters::g3);
+                m_assembler.add_imm(SparcRegisters::g3, 4, SparcRegisters::g3);
+                m_assembler.stf_r(src + 1, address.base, SparcRegisters::g3);
+            }
+        }
+
+        void storeDouble(ImmDouble imm, Address address)
+        {
+            store32(Imm32(imm.u.s.msb), address);
+            store32(Imm32(imm.u.s.lsb), Address(address.base, address.offset + 4));
+        }
+
+        void storeDouble(ImmDouble imm, BaseIndex address)
+        {
+            store32(Imm32(imm.u.s.msb), address);
+            store32(Imm32(imm.u.s.lsb),
+                    BaseIndex(address.base, address.index, address.scale, address.offset + 4));
+        }
+
+        void addDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.faddd_r(src, dest, dest);
+        }
+
+        void addDouble(Address src, FPRegisterID dest)
+        {
+            loadDouble(src, SparcRegisters::f30);
+            m_assembler.faddd_r(SparcRegisters::f30, dest, dest);
+        }
+
+        void divDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fdivd_r(dest, src, dest);
+        }
+
+        void divDouble(Address src, FPRegisterID dest)
+        {
+            loadDouble(src, SparcRegisters::f30);
+            m_assembler.fdivd_r(dest, SparcRegisters::f30, dest);
+        }
+
+        void subDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fsubd_r(dest, src, dest);
+        }
+
+        void subDouble(Address src, FPRegisterID dest)
+        {
+            loadDouble(src, SparcRegisters::f30);
+            m_assembler.fsubd_r(dest, SparcRegisters::f30, dest);
+        }
+
+        void mulDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fmuld_r(src, dest, dest);
+        }
+
+        void mulDouble(Address src, FPRegisterID dest)
+        {
+            loadDouble(src, SparcRegisters::f30);
+            m_assembler.fmuld_r(SparcRegisters::f30, dest, dest);
+        }
+
+        void sqrtDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fsqrtd_r(src, dest);
+        }
+
+        void negDouble(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fnegd_r(src, dest);
+        }
+
+        void convertUInt32ToDouble(RegisterID src, FPRegisterID dest)
+        {
+            m_assembler.move_nocheck(0x43300000, SparcRegisters::g1);
+            m_assembler.stw_imm(SparcRegisters::g1, SparcRegisters::sp, 0x60);
+            m_assembler.stw_imm(src, SparcRegisters::sp, 0x64);
+            m_assembler.lddf_imm(SparcRegisters::sp, 0x60, SparcRegisters::f30);
+            m_assembler.stw_imm(SparcRegisters::g0, SparcRegisters::sp, 0x64);
+            m_assembler.lddf_imm(SparcRegisters::sp, 0x60, dest);
+            m_assembler.fsubd_r(SparcRegisters::f30, dest, dest);
+            m_assembler.fabss_r(dest, dest);
+        }
+
+        void convertInt32ToDouble(RegisterID src, FPRegisterID dest)
+        {
+            m_assembler.stw_imm(src, SparcRegisters::sp, 0x60);
+            m_assembler.ldf_imm(SparcRegisters::sp, 0x60, dest);
+            m_assembler.fitod_r(dest, dest);
+        }
+
+        void convertInt32ToDouble(Address address, FPRegisterID dest)
+        {
+            if (m_assembler.isimm13(address.offset))
+                m_assembler.ldf_imm(address.base, address.offset, dest);
+            else {
+                m_assembler.move_nocheck(address.offset, SparcRegisters::g3);
+                m_assembler.ldf_r(address.base, SparcRegisters::g3, dest);
+            }
+            m_assembler.fitod_r(dest, dest);
+        }
+
+        void convertInt32ToDouble(AbsoluteAddress src, FPRegisterID dest)
+        {
+            m_assembler.move_nocheck((int)src.m_ptr, SparcRegisters::g3);
+            m_assembler.ldf_r(SparcRegisters::g3, SparcRegisters::g0, dest);
+            m_assembler.fitod_r(dest, dest);
+        }
+
+        void fastLoadDouble(RegisterID lo, RegisterID hi, FPRegisterID fpReg)
+        {
+            m_assembler.stw_imm(lo, SparcRegisters::sp, 0x64);
+            m_assembler.stw_imm(hi, SparcRegisters::sp, 0x60);
+            m_assembler.lddf_imm(SparcRegisters::sp, 0x60, fpReg);
+        }
+
+        void convertDoubleToFloat(FPRegisterID src, FPRegisterID dest)
+        {
+            m_assembler.fdtos_r(src, dest);
+        }
+
+        void breakDoubleTo32(FPRegisterID srcDest, RegisterID typeReg, RegisterID dataReg) {
+            m_assembler.stdf_imm(srcDest, SparcRegisters::sp, 0x60);
+            m_assembler.lduw_imm(SparcRegisters::sp, 0x60, typeReg);
+            m_assembler.lduw_imm(SparcRegisters::sp, 0x64, dataReg);
+        }
+
+        Jump branchDouble(DoubleCondition cond, FPRegisterID left, FPRegisterID right)
+        {
+            m_assembler.fcmpd_r(left, right);
+            return Jump(m_assembler.fbranch(SparcDoubleCondition(cond)));
+        }
+
+        // Truncates 'src' to an integer, and places the resulting 'dest'.
+        // If the result is not representable as a 32 bit value, branch.
+        // May also branch for some values that are representable in 32 bits
+        // (specifically, in this case, INT_MIN).
+        Jump branchTruncateDoubleToInt32(FPRegisterID src, RegisterID dest)
+        {
+            m_assembler.fdtoi_r(src, SparcRegisters::f30);
+            m_assembler.stf_imm(SparcRegisters::f30, SparcRegisters::sp, 0x60);
+            m_assembler.lduw_imm(SparcRegisters::sp, 0x60, dest);
+
+            m_assembler.or_r(SparcRegisters::g0, SparcRegisters::g0, SparcRegisters::g2);
+            m_assembler.move_nocheck(0x80000000, SparcRegisters::g3);
+            m_assembler.subcc_r(SparcRegisters::g3, dest, SparcRegisters::g0);
+            m_assembler.movcc_imm(1, SparcRegisters::g2, SparcCondition(Equal));
+            m_assembler.move_nocheck(0x7fffffff, SparcRegisters::g3);
+            m_assembler.subcc_r(SparcRegisters::g3, dest, SparcRegisters::g0);
+            m_assembler.movcc_imm(1, SparcRegisters::g2, SparcCondition(Equal));
+
+            return branch32(Equal, SparcRegisters::g2, Imm32(1));
+        }
+
+        // Convert 'src' to an integer, and places the resulting 'dest'.
+        // If the result is not representable as a 32 bit value, branch.
+        // May also branch for some values that are representable in 32 bits
+        // (specifically, in this case, 0).
+        void branchConvertDoubleToInt32(FPRegisterID src, RegisterID dest, JumpList& failureCases, FPRegisterID fpTemp)
+        {
+            m_assembler.fdtoi_r(src, SparcRegisters::f30);
+            m_assembler.stf_imm(SparcRegisters::f30, SparcRegisters::sp, 0x60);
+            m_assembler.lduw_imm(SparcRegisters::sp, 0x60, dest);
+
+            // Convert the integer result back to float & compare to the original value - if not equal or unordered (NaN) then jump.
+            m_assembler.fitod_r(SparcRegisters::f30, SparcRegisters::f30);
+            failureCases.append(branchDouble(DoubleNotEqualOrUnordered, src, SparcRegisters::f30));
+
+            // If the result is zero, it might have been -0.0, and 0.0 equals to -0.0
+            failureCases.append(branchTest32(Zero, dest));
+        }
+
+        void zeroDouble(FPRegisterID srcDest)
+        {
+            m_assembler.fsubd_r(srcDest, srcDest, srcDest);
+        }
+
+    protected:
+        SparcAssembler::Condition SparcCondition(Condition cond)
+        {
+            return static_cast<SparcAssembler::Condition>(cond);
+        }
+
+        SparcAssembler::DoubleCondition SparcDoubleCondition(DoubleCondition cond)
+        {
+            return static_cast<SparcAssembler::DoubleCondition>(cond);
+        }
+
+    private:
+        friend class LinkBuffer;
+        friend class RepatchBuffer;
+
+        static void linkCall(void* code, Call call, FunctionPtr function)
+        {
+            SparcAssembler::linkCall(code, call.m_jmp, function.value());
+        }
+
+        static void repatchCall(CodeLocationCall call, CodeLocationLabel destination)
+        {
+            SparcAssembler::relinkCall(call.dataLocation(), destination.executableAddress());
+        }
+
+        static void repatchCall(CodeLocationCall call, FunctionPtr destination)
+        {
+            SparcAssembler::relinkCall(call.dataLocation(), destination.executableAddress());
+        }
+
+    };
+
+}
+
+
+#endif // ENABLE(ASSEMBLER) && CPU(SPARC)
+
+#endif // MacroAssemblerSparc_h
diff --git a/js/src/assembler/assembler/SparcAssembler.h b/js/src/assembler/assembler/SparcAssembler.h
new file mode 100644
index 0000000..72f03fa
--- /dev/null
+++ b/assembler/assembler/SparcAssembler.h
@@ -0,0 +1,1289 @@
+/* -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
+ * vim: set ts=4 sw=4 et tw=99:
+ *
+ * ***** BEGIN LICENSE BLOCK *****
+ * Version: MPL 1.1/GPL 2.0/LGPL 2.1
+ *
+ * The contents of this file are subject to the Mozilla Public License Version
+ * 1.1 (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ * http://www.mozilla.org/MPL/
+ *
+ * Software distributed under the License is distributed on an "AS IS" basis,
+ * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
+ * for the specific language governing rights and limitations under the
+ * License.
+ *
+ * The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
+ * May 28, 2008.
+ *
+ * The Initial Developer of the Original Code is
+ * Leon Sha <leon.sha@oracle.com>
+ * 
+ * Portions created by the Initial Developer are Copyright (C) 2010-2011
+ * the Initial Developer. All Rights Reserved.
+ *
+ * Contributor(s):
+ *
+ * Alternatively, the contents of this file may be used under the terms of
+ * either the GNU General Public License Version 2 or later (the "GPL"), or
+ * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
+ * in which case the provisions of the GPL or the LGPL are applicable instead
+ * of those above. If you wish to allow use of your version of this file only
+ * under the terms of either the GPL or the LGPL, and not to allow others to
+ * use your version of this file under the terms of the MPL, indicate your
+ * decision by deleting the provisions above and replace them with the notice
+ * and other provisions required by the GPL or the LGPL. If you do not delete
+ * the provisions above, a recipient may use your version of this file under
+ * the terms of any one of the MPL, the GPL or the LGPL.
+ *
+ * ***** END LICENSE BLOCK ***** */
+
+#ifndef SparcAssembler_h
+#define SparcAssembler_h
+
+#include <wtf/Platform.h>
+
+// Some debug code uses s(n)printf for instruction logging.
+#include <stdio.h>
+
+#if ENABLE_ASSEMBLER && WTF_CPU_SPARC
+
+#include "AssemblerBufferWithConstantPool.h"
+#include <wtf/Assertions.h>
+
+#include "methodjit/Logging.h"
+#define IPFX  "        %s"
+#define ISPFX "        "
+#ifdef JS_METHODJIT_SPEW
+# define MAYBE_PAD (isOOLPath ? ">  " : "")
+# define PRETTY_PRINT_OFFSET(os) (((os)<0)?"-":""), (((os)<0)?-(os):(os))
+# define FIXME_INSN_PRINTING                                \
+    do {                                                    \
+        js::JaegerSpew(js::JSpew_Insns,                     \
+                       ISPFX "FIXME insn printing %s:%d\n", \
+                       __FILE__, __LINE__);                 \
+    } while (0)
+#else
+# define MAYBE_PAD ""
+# define FIXME_INSN_PRINTING ((void) 0)
+# define PRETTY_PRINT_OFFSET(os) "", 0
+#endif
+
+namespace JSC {
+
+    typedef uint32_t SparcWord;
+
+    namespace SparcRegisters {
+        typedef enum {
+            g0 = 0, // g0 is always 0
+            g1 = 1, // g1 is a scratch register for v8
+            g2 = 2,
+            g3 = 3,
+            g4 = 4,
+            g5 = 5, // Reserved for system
+            g6 = 6, // Reserved for system
+            g7 = 7, // Reserved for system
+
+            o0 = 8,
+            o1 = 9,
+            o2 = 10,
+            o3 = 11,
+            o4 = 12,
+            o5 = 13,
+            o6 = 14, // SP
+            o7 = 15,
+
+            l0 = 16,
+            l1 = 17,
+            l2 = 18,
+            l3 = 19,
+            l4 = 20,
+            l5 = 21,
+            l6 = 22,
+            l7 = 23,
+
+            i0 = 24,
+            i1 = 25,
+            i2 = 26,
+            i3 = 27,
+            i4 = 28,
+            i5 = 29,
+            i6 = 30, // FP
+            i7 = 31,
+
+            sp = o6,
+            fp = i6
+        } RegisterID;
+
+        typedef enum {
+            f0 = 0,
+            f1 = 1,
+            f2 = 2,
+            f3 = 3,
+            f4 = 4,
+            f5 = 5,
+            f6 = 6,
+            f7 = 7,
+            f8 = 8,
+            f9 = 9,
+            f10 = 10,
+            f11 = 11,
+            f12 = 12,
+            f13 = 13,
+            f14 = 14,
+            f15 = 15,
+            f16 = 16,
+            f17 = 17,
+            f18 = 18,
+            f19 = 19,
+            f20 = 20,
+            f21 = 21,
+            f22 = 22,
+            f23 = 23,
+            f24 = 24,
+            f25 = 25,
+            f26 = 26,
+            f27 = 27,
+            f28 = 28,
+            f29 = 29,
+            f30 = 30,
+            f31 = 31
+        } FPRegisterID;
+
+    } // namespace SparcRegisters
+
+    class SparcAssembler {
+    public:
+        typedef SparcRegisters::RegisterID RegisterID;
+        typedef SparcRegisters::FPRegisterID FPRegisterID;
+        AssemblerBuffer m_buffer;
+        bool oom() const { return m_buffer.oom(); }
+
+#ifdef JS_METHODJIT_SPEW
+        bool isOOLPath;
+        SparcAssembler() : isOOLPath(false) { }
+#else
+        SparcAssembler() { }
+#endif
+
+        // Sparc conditional constants
+        typedef enum {
+            ConditionE   = 0x1, // Zero
+            ConditionLE  = 0x2,
+            ConditionL   = 0x3,
+            ConditionLEU = 0x4,
+            ConditionCS  = 0x5,
+            ConditionNEG = 0x6,
+            ConditionVS  = 0x7,
+            ConditionA   = 0x8, // branch_always
+            ConditionNE  = 0x9, // Non-zero
+            ConditionG   = 0xa,
+            ConditionGE  = 0xb,
+            ConditionGU  = 0xc,
+            ConditionCC  = 0xd,
+            ConditionVC  = 0xf
+        } Condition;
+
+
+        typedef enum {
+            DoubleConditionNE  = 0x1,
+            DoubleConditionUL  = 0x3,
+            DoubleConditionL   = 0x4,
+            DoubleConditionUG  = 0x5,
+            DoubleConditionG   = 0x6,
+            DoubleConditionE   = 0x9,
+            DoubleConditionUE  = 0xa,
+            DoubleConditionGE  = 0xb,
+            DoubleConditionUGE = 0xc,
+            DoubleConditionLE  = 0xd,
+            DoubleConditionULE = 0xe
+        } DoubleCondition;
+
+        typedef enum {
+            BranchOnCondition,
+            BranchOnDoubleCondition
+        } BranchType;
+
+        class JmpSrc {
+            friend class SparcAssembler;
+        public:
+            JmpSrc()
+                : m_offset(-1)
+            {
+            }
+
+        private:
+            JmpSrc(int offset)
+                : m_offset(offset)
+            {
+            }
+
+            int m_offset;
+        };
+
+        class JmpDst {
+            friend class SparcAssembler;
+        public:
+            JmpDst()
+                : m_offset(-1)
+                , m_used(false)
+            {
+            }
+
+            bool isUsed() const { return m_used; }
+            void used() { m_used = true; }
+            bool isValid() const { return m_offset != -1; }
+        private:
+            JmpDst(int offset)
+                : m_offset(offset)
+                , m_used(false)
+            {
+                ASSERT(m_offset == offset);
+            }
+
+            int m_used : 1;
+            signed int m_offset : 31;
+        };
+
+        // Instruction formating
+
+        void format_2_1(int rd, int op2, int imm22)
+        {
+            m_buffer.putInt(rd << 25 | op2 << 22 | (imm22 & 0x3FFFFF));
+        }
+
+        void format_2_2(int a, int cond, int op2, int disp22)
+        {
+            format_2_1((a & 0x1) << 4 | (cond & 0xF), op2, disp22);
+        }
+
+        void format_2_3(int a, int cond, int op2, int cc1, int cc0, int p, int disp19)
+        {
+            format_2_2(a, cond, op2, (cc1 & 0x1) << 21 | (cc0 & 0x1) << 20 | (p & 0x1) << 19 | (disp19 & 0x7FFFF));
+        }
+
+        void format_2_4(int a, int rcond, int op2, int d16hi, int p, int rs1, int d16lo)
+        {
+            format_2_2(a, (rcond & 0x7), op2, (d16hi & 0x3) << 20 | (p & 0x1) << 19 | rs1 << 14 | (d16lo & 0x3FFF));
+        }
+
+        void format_3(int op1, int rd, int op3, int bits19)
+        {
+            m_buffer.putInt(op1 << 30 | rd << 25 | op3 << 19 | (bits19 & 0x7FFFF));
+        }
+
+        void format_3_1(int op1, int rd, int op3, int rs1, int bit8, int rs2)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | (bit8 & 0xFF) << 5 | rs2);
+        }
+
+        void format_3_1_imm(int op1, int rd, int op3, int rs1, int simm13)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | 1 << 13 | (simm13 & 0x1FFF));
+        }
+
+        void format_3_2(int op1, int rd, int op3, int rs1, int rcond, int rs2)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | (rcond & 0x3) << 10 | rs2);
+        }
+
+        void format_3_2_imm(int op1, int rd, int op3, int rs1, int rcond, int simm10)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | 1 << 13 | (rcond & 0x3) << 10 | (simm10 & 0x1FFF));
+        }
+
+        void format_3_3(int op1, int rd, int op3, int rs1, int cmask, int mmask)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | 1 << 13 | (cmask & 0x7) << 5 | (mmask & 0xF));
+        }
+        void format_3_4(int op1, int rd, int op3, int bits19)
+        {
+            format_3(op1, rd, op3, bits19);
+        }
+
+        void format_3_5(int op1, int rd, int op3, int rs1, int x, int rs2)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | (x & 0x1) << 12 | rs2);
+        }
+
+        void format_3_6(int op1, int rd, int op3, int rs1, int shcnt32)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | 1 << 13 | (shcnt32 & 0x1F));
+        }
+
+        void format_3_7(int op1, int rd, int op3, int rs1, int shcnt64)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | 1 << 13 | 1 << 12 | (shcnt64 & 0x3F));
+        }
+
+        void format_3_8(int op1, int rd, int op3, int rs1, int bits9, int rs2)
+        {
+            format_3(op1, rd, op3, rs1 << 14 | (bits9 & 0x1FF) << 5 | rs2);
+        }
+
+        void format_3_9(int op1, int cc1, int cc0, int op3, int rs1, int bits9, int rs2)
+        {
+            format_3(op1, (cc1 & 0x1) << 1 | (cc0 & 0x1), op3, rs1 << 14 | (bits9 & 0x1FF) << 5 | rs2);
+        }
+
+        void format_4_1(int rd, int op3, int rs1, int cc1, int cc0, int rs2)
+        {
+            format_3(2, rd, op3, rs1 << 14 | (cc1 & 0x1) << 12 | (cc0 & 0x1) << 11 | rs2);
+        }
+
+        void format_4_1_imm(int rd, int op3, int rs1, int cc1, int cc0, int simm11)
+        {
+            format_3(2, rd, op3, rs1 << 14 | (cc1 & 0x1) << 12 | 1 << 13 |(cc0 & 0x1) << 11 | (simm11 & 0x7FF));
+        }
+
+        void format_4_2(int rd, int op3, int cc2, int cond, int cc1, int cc0, int rs2)
+        {
+            format_3(2, rd, op3, (cc2 & 0x1) << 18 | (cond & 0xF) << 14 | (cc1 & 0x1) << 12 | (cc0 & 0x1) << 11 | rs2);
+        }
+
+        void format_4_2_imm(int rd, int op3, int cc2, int cond, int cc1, int cc0, int simm11)
+        {
+            format_3(2, rd, op3, (cc2 & 0x1) << 18 | (cond & 0xF) << 14 | 1 << 13 | (cc1 & 0x1) << 12 | (cc0 & 0x1) << 11 | (simm11 & 0x7FF));
+        }
+
+        void format_4_3(int rd, int op3, int rs1, int cc1, int cc0, int swap_trap)
+        {
+            format_3(2, rd, op3, rs1 << 14 | 1 << 13 | (cc1 & 0x1) << 12 | (cc0 & 0x1) << 11 | (swap_trap & 0x7F));
+        }
+
+        void format_4_4(int rd, int op3, int rs1, int rcond, int opf_low, int rs2)
+        {
+            format_3(2, rd, op3, rs1 << 14 | (rcond & 0x7) << 10 | (opf_low & 0x1F) << 5 | rs2);
+        }
+
+        void format_4_5(int rd, int op3, int cond, int opf_cc, int opf_low, int rs2)
+        {
+            format_3(2, rd, op3, (cond & 0xF) << 14 | (opf_cc & 0x7) << 11 | (opf_low & 0x3F) << 5 | rs2);
+        }
+
+        void addcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "addcc       %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x10, rs1, 0, rs2);
+        }
+
+        void addcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "addcc       %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x10, rs1, simm13);
+        }
+
+        void add_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "add         %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0, rs1, 0, rs2);
+        }
+
+        void add_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "add         %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0, rs1, simm13);
+        }
+
+        void andcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "andcc       %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x11, rs1, 0, rs2);
+        }
+
+        void andcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "andcc       %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x11, rs1, simm13);
+        }
+
+        void or_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "or          %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x2, rs1, 0, rs2);
+        }
+
+        void or_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "or          %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x2, rs1, simm13);
+        }
+
+        // sethi %hi(imm22) rd
+        void sethi(int imm22, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sethi       %%hi(0x%x), %s\n", MAYBE_PAD,
+                           imm22, nameGpReg(rd));
+            format_2_1(rd, 0x4, (imm22 >> 10));
+        }
+
+        void sll_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sll         %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_5(2, rd, 0x25, rs1, 0, rs2);
+        }
+
+        void sll_imm(int rs1, int shcnt32, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sll         %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), shcnt32, nameGpReg(rd));
+            format_3_6(2, rd, 0x25, rs1, shcnt32);
+        }
+
+        void sra_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sra         %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_5(2, rd, 0x27, rs1, 0, rs2);
+        }
+
+        void sra_imm(int rs1, int shcnt32, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sra         %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), shcnt32, nameGpReg(rd));
+            format_3_6(2, rd, 0x27, rs1, shcnt32);
+        }
+
+        void srl_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "srl         %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_5(2, rd, 0x26, rs1, 0, rs2);
+        }
+
+        void srl_imm(int rs1, int shcnt32, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "srl         %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), shcnt32, nameGpReg(rd));
+            format_3_6(2, rd, 0x26, rs1, shcnt32);
+        }
+
+        void subcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "subcc       %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x14, rs1, 0, rs2);
+        }
+
+        void subcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "subcc       %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x14, rs1, simm13);
+        }
+
+        void orcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "orcc        %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x12, rs1, 0, rs2);
+        }
+
+        void orcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "orcc        %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x12, rs1, simm13);
+        }
+
+        void xorcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "xorcc       %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x13, rs1, 0, rs2);
+        }
+
+        void xorcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "xorcc       %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x13, rs1, simm13);
+        }
+
+        void xnorcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "xnorcc      %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x17, rs1, 0, rs2);
+        }
+
+        void xnorcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "xnorcc      %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x17, rs1, simm13);
+        }
+
+        void smulcc_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "smulcc      %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x1b, rs1, 0, rs2);
+        }
+
+        void smulcc_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "smulcc      %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x1b, rs1, simm13);
+        }
+
+        void ldsb_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldsb        [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0x9, rs1, 0, rs2);
+        }
+
+        void ldsb_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldsb        [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(3, rd, 0x9, rs1, simm13);
+        }
+
+        void ldub_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldub        [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0x1, rs1, 0, rs2);
+        }
+
+        void ldub_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldub        [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(3, rd, 0x1, rs1, simm13);
+        }
+
+        void lduw_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "lduw        [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0x0, rs1, 0, rs2);
+        }
+
+        void lduwa_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "lduwa       [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0x10, rs1, 0x82, rs2);
+        }
+
+        void lduw_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "lduw        [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(3, rd, 0x0, rs1, simm13);
+        }
+
+        void ldsh_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldsh        [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0xa, rs1, 0, rs2);
+        }
+
+        void ldsh_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldsh        [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(3, rd, 0xa, rs1, simm13);
+        }
+
+        void lduh_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "lduh        [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(3, rd, 0x2, rs1, 0, rs2);
+        }
+
+        void lduh_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "lduh        [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(3, rd, 0x2, rs1, simm13);
+        }
+
+        void stb_r(int rd, int rs2, int rs1)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "stb         %s, [%s + %s]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), nameGpReg(rs2));
+            format_3_1(3, rd, 0x5, rs1, 0, rs2);
+        }
+
+        void stb_imm(int rd, int rs1, int simm13)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "stb         %s, [%s + %d]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), simm13);
+            format_3_1_imm(3, rd, 0x5, rs1, simm13);
+        }
+
+        void sth_r(int rd, int rs2, int rs1)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sth         %s, [%s + %s]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), nameGpReg(rs2));
+            format_3_1(3, rd, 0x6, rs1, 0, rs2);
+        }
+
+        void sth_imm(int rd, int rs1, int simm13)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "sth         %s, [%s + %d]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), simm13);
+            format_3_1_imm(3, rd, 0x6, rs1, simm13);
+        }
+
+        void stw_r(int rd, int rs2, int rs1)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "stw         %s, [%s + %s]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), nameGpReg(rs2));
+            format_3_1(3, rd, 0x4, rs1, 0, rs2);
+        }
+
+        void stw_imm(int rd, int rs1, int simm13)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "stw         %s, [%s + %d]\n", MAYBE_PAD,
+                           nameGpReg(rd), nameGpReg(rs1), simm13);
+            format_3_1_imm(3, rd, 0x4, rs1, simm13);
+        }
+
+        void ldf_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ld          [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameFpReg(rd));
+            format_3_1(3, rd, 0x20, rs1, 0, rs2);
+        }
+
+        void ldf_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ld          [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameFpReg(rd));
+            format_3_1_imm(3, rd, 0x20, rs1, simm13);
+        }
+
+        // lddf and stdf will have mem align problem.
+        void lddf_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldd         [%s + %s], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameFpReg(rd));
+            format_3_1(3, rd, 0x23, rs1, 0, rs2);
+        }
+
+        void lddf_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ldd         [%s + %d], %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameFpReg(rd));
+            format_3_1_imm(3, rd, 0x23, rs1, simm13);
+        }
+
+        void stdf_r(int rd, int rs2, int rs1)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "std         %s, [%s + %s]\n", MAYBE_PAD,
+                           nameFpReg(rd), nameGpReg(rs1), nameGpReg(rs2));
+            format_3_1(3, rd, 0x27, rs1, 0, rs2);
+        }
+
+        void stdf_imm(int rd, int rs1, int simm13)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "std         %s, [%s + %d]\n", MAYBE_PAD,
+                           nameFpReg(rd), nameGpReg(rs1), simm13);
+            format_3_1_imm(3, rd, 0x27, rs1, simm13);
+        }
+
+        void stf_r(int rd, int rs2, int rs1)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "st          %s, [%s + %s]\n", MAYBE_PAD,
+                           nameFpReg(rd), nameGpReg(rs1), nameGpReg(rs2));
+            format_3_1(3, rd, 0x24, rs1, 0, rs2);
+        }
+
+        void stf_imm(int rd, int rs1, int simm13)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "st          %s, [%s + %d]\n", MAYBE_PAD,
+                           nameFpReg(rd), nameGpReg(rs1), simm13);
+            format_3_1_imm(3, rd, 0x24, rs1, simm13);
+        }
+
+        void fmovd_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fmovd       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0x2, rs2);
+        }
+
+        void fcmpd_r(int rs1, int rs2)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fcmpd       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs1), nameFpReg(rs2));
+            format_3_9(2, 0, 0, 0x35, rs1, 0x52, rs2);
+        }
+
+        void nop()
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "nop\n", MAYBE_PAD);
+            format_2_1(0, 0x4, 0);
+        }
+
+        void branch_con(Condition cond, int target)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "b%s         0x%x\n", MAYBE_PAD,
+                           nameICC(cond), target);
+            format_2_2(0, cond, 0x2, target);
+        }
+
+        void fbranch_con(DoubleCondition cond, int target)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fb%s        0x%x\n", MAYBE_PAD,
+                           nameFCC(cond), target);
+            format_2_2(0, cond, 0x6, target);
+        }
+
+        void rdy(int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "rdy         %s\n", MAYBE_PAD,
+                           nameFpReg(rd));
+            format_3_1(2, rd, 0x28, 0, 0, 0);
+        }
+
+        void rdpc(int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "rdpc        %s\n", MAYBE_PAD,
+                           nameGpReg(rd));
+            format_3_1(2, rd, 0x28, 5, 0, 0);
+        }
+        void jmpl_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "jmpl        %s + %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x38, rs1, 0, rs2);
+        }
+
+        void jmpl_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "jmpl        %s + %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x38, rs1, simm13);
+        }
+
+        void save_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "save        %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x3c, rs1, 0, rs2);
+        }
+
+        void save_imm(int rs1, int simm13, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "save        %s, %d, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), simm13, nameGpReg(rd));
+            format_3_1_imm(2, rd, 0x3c, rs1, simm13);
+        }
+
+        void restore_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "restore     %s, %s, %s\n", MAYBE_PAD,
+                           nameGpReg(rs1), nameGpReg(rs2), nameGpReg(rd));
+            format_3_1(2, rd, 0x3d, rs1, 0, rs2);
+        }
+
+        void ta_imm(int swap_trap)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "ta          %d\n", MAYBE_PAD,
+                           swap_trap);
+            format_4_3(0x8, 0xa, 0, 0, 0, swap_trap);
+        }
+
+        void movcc_imm(int simm11, int rd, Condition cond)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "mov%s       %d, %s\n", MAYBE_PAD,
+                           nameICC(cond), simm11, nameGpReg(rd));
+            format_4_2_imm(rd, 0x2c, 1, cond, 0, 0, simm11);
+        }
+
+        void fabss_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fabss       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0x9, rs2);
+        }
+
+        void faddd_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "faddd       %s, %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs1), nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, rs1, 0x42, rs2);
+        }
+
+        void fsubd_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fsubd       %s, %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs1), nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, rs1, 0x46, rs2);
+        }
+
+        void fmuld_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fmuld       %s, %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs1), nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, rs1, 0x4a, rs2);
+        }
+
+        void fdivd_r(int rs1, int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fdivd       %s, %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs1), nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, rs1, 0x4e, rs2);
+        }
+
+        void fsqrtd_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fsqartd     %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0x2a, rs2);
+        }
+
+        void fnegd_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fnegd       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0x06, rs2);
+        }
+
+        void fitod_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fitod       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0xc8, rs2);
+        }
+
+        void fdtoi_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fdtoi       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0xd2, rs2);
+        }
+
+        void fdtos_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fdtos       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0xc6, rs2);
+        }
+
+        void fstod_r(int rs2, int rd)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "fstod       %s, %s\n", MAYBE_PAD,
+                           nameFpReg(rs2), nameFpReg(rd));
+            format_3_8(2, rd, 0x34, 0, 0xc9, rs2);
+        }
+
+        static bool isimm13(int imm)
+        {
+            return (imm) <= 0xfff && (imm) >= -0x1000;
+        }
+
+        static bool isimm22(int imm)
+        {
+            return (imm) <= 0x1fffff && (imm) >= -0x200000;
+        }
+
+        void move_nocheck(int imm_v, RegisterID dest)
+        {
+            sethi(imm_v, dest);
+            or_imm(dest, imm_v & 0x3FF, dest);
+        }
+
+        JmpSrc call()
+        {
+            JmpSrc r = JmpSrc(m_buffer.size());
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "call        %d\n", MAYBE_PAD,
+                           r.m_offset);
+            m_buffer.putInt(0x40000000);
+            nop();
+            return r;
+        }
+
+        JmpSrc jump_common(BranchType branchtype, int cond)
+        {
+            if (branchtype == BranchOnCondition)
+                branch_con(Condition(cond), 0);
+            else
+                fbranch_con(DoubleCondition(cond), 0);
+
+            nop();
+            branch_con(ConditionA, 7);
+            nop();
+            move_nocheck(0, SparcRegisters::g2);
+            rdpc(SparcRegisters::g3);
+            jmpl_r(SparcRegisters::g2, SparcRegisters::g3, SparcRegisters::g0);
+            nop();
+            return JmpSrc(m_buffer.size());
+        }
+
+        JmpSrc branch(Condition cond)
+        {
+            return jump_common(BranchOnCondition, cond);
+        }
+
+        JmpSrc fbranch(DoubleCondition cond)
+        {
+            return jump_common(BranchOnDoubleCondition, cond);
+        }
+
+        JmpSrc jmp()
+        {
+            return jump_common(BranchOnCondition, ConditionA);
+        }
+
+        // Assembler admin methods:
+
+        JmpDst label()
+        {
+            JmpDst r = JmpDst(m_buffer.size());
+            js::JaegerSpew(js::JSpew_Insns,
+                           IPFX "#label     ((%d))\n", MAYBE_PAD, r.m_offset);
+            return r;
+        }
+
+        // General helpers
+
+        size_t size() const { return m_buffer.size(); }
+        unsigned char *buffer() const { return m_buffer.buffer(); }
+
+        static int getDifferenceBetweenLabels(JmpDst src, JmpDst dst)
+        {
+            return dst.m_offset - src.m_offset;
+        }
+    
+        static int getDifferenceBetweenLabels(JmpDst src, JmpSrc dst)
+        {
+            return dst.m_offset - src.m_offset;
+        }
+    
+        static int getDifferenceBetweenLabels(JmpSrc src, JmpDst dst)
+        {
+            return dst.m_offset - src.m_offset;
+        }
+
+        static unsigned getCallReturnOffset(JmpSrc call)
+        {
+            return call.m_offset + 20;
+        }
+
+        static void* getRelocatedAddress(void* code, JmpSrc jump)
+        {
+            ASSERT(jump.m_offset != -1);
+
+            return reinterpret_cast<void*>(reinterpret_cast<ptrdiff_t>(code) + jump.m_offset);
+        }
+    
+        static void* getRelocatedAddress(void* code, JmpDst destination)
+        {
+            ASSERT(destination.m_offset != -1);
+
+            return reinterpret_cast<void*>(reinterpret_cast<ptrdiff_t>(code) + destination.m_offset);
+        }
+
+        void* executableCopy(ExecutablePool* allocator)
+        {
+            return m_buffer.executableCopy(allocator);
+        }
+
+        void* executableCopy(void* buffer)
+        {
+            return memcpy(buffer, m_buffer.buffer(), size());
+        }
+
+        static void patchPointerInternal(void* where, int value)
+        {
+            // Patch move_nocheck.
+            uint32_t *branch = (uint32_t*) where;
+            branch[0] &= 0xFFC00000;
+            branch[0] |= (value >> 10) & 0x3FFFFF;
+            branch[1] &= 0xFFFFFC00;
+            branch[1] |= value & 0x3FF;
+            ExecutableAllocator::cacheFlush(where, 8);
+        }
+
+        static void patchbranch(void* where, int value)
+        {
+            uint32_t *branch = (uint32_t*) where;
+            branch[0] &= 0xFFC00000;
+            branch[0] |= value & 0x3FFFFF;
+            ExecutableAllocator::cacheFlush(where, 4);
+        }
+
+        static bool canRelinkJump(void* from, void* to)
+        {
+            return true;
+        }
+
+        static void relinkJump(void* from, void* to)
+        {
+            from = (void *)((int)from - 36);
+            js::JaegerSpew(js::JSpew_Insns,
+                           ISPFX "##link     ((%p)) jumps to ((%p))\n",
+                           from, to);
+
+            int value = ((int)to - (int)from) / 4;
+            if (isimm22(value)) 
+                patchbranch(from, value);
+            else {
+                patchbranch(from, 4);
+                from = (void *)((intptr_t)from + 16);
+                patchPointerInternal(from, (int)(value * 4 - 24));
+            }
+        }
+
+        void linkJump(JmpSrc from, JmpDst to)
+        {
+            ASSERT(from.m_offset != -1);
+            ASSERT(to.m_offset != -1);
+            intptr_t code = (intptr_t)(m_buffer.data());
+            void *where = (void *)((intptr_t)code + from.m_offset);
+            void *target = (void *)((intptr_t)code + to.m_offset);
+            relinkJump(where, target);
+        }
+
+        static void linkJump(void* code, JmpSrc from, void* to)
+        {
+            ASSERT(from.m_offset != -1);
+            void *where = (void *)((intptr_t)code + from.m_offset);
+            relinkJump(where, to);
+        }
+
+        static void relinkCall(void* from, void* to)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           ISPFX "##linkCall ((from=%p)) ((to=%p))\n",
+                           from, to);
+
+            int disp = ((int)to - (int)from)/4;
+            *(uint32_t *)((int)from) &= 0x40000000;
+            *(uint32_t *)((int)from) |= disp & 0x3fffffff;
+            ExecutableAllocator::cacheFlush(from, 4);
+        }
+
+        static void linkCall(void* code, JmpSrc where, void* to)
+        {
+            void *from = (void *)((intptr_t)code + where.m_offset);
+            relinkCall(from, to);
+        }
+
+        static void linkPointer(void* code, JmpDst where, void* value)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           ISPFX "##linkPointer     ((%p + %#x)) points to ((%p))\n",
+                           code, where.m_offset, value);
+
+            void *from = (void *)((intptr_t)code + where.m_offset);
+            patchPointerInternal(from, (int)value);
+        }
+
+        static void repatchInt32(void* where, int value)
+        {
+            js::JaegerSpew(js::JSpew_Insns,
+                           ISPFX "##repatchInt32 ((where=%p)) holds ((value=%d))\n",
+                           where, value);
+
+            patchPointerInternal(where, value);
+        }
+
+        static void repatchPointer(void* where, void* value)
+        { 
+            js::JaegerSpew(js::JSpew_Insns,
+                           ISPFX "##repatchPointer ((where = %p)) points to ((%p))\n",
+                           where, value);
+
+            patchPointerInternal(where, (int)value);
+        }
+
+        static void repatchLoadPtrToLEA(void* where)
+        {
+            // sethi is used. The offset is in a register
+            if (*(uint32_t *)((int)where) & 0x01000000)
+                where = (void *)((intptr_t)where + 8);
+
+            *(uint32_t *)((int)where) &= 0x3fffffff;
+            *(uint32_t *)((int)where) |= 0x80000000;
+            ExecutableAllocator::cacheFlush(where, 4);
+        }
+
+        static void repatchLEAToLoadPtr(void* where)
+        {
+            // sethi is used. The offset is in a register
+            if (*(uint32_t *)((int)where) & 0x01000000)
+                where = (void *)((intptr_t)where + 8);
+
+            *(uint32_t *)((int)where) &= 0x3fffffff;
+            *(uint32_t *)((int)where) |= 0xc0000000;
+            ExecutableAllocator::cacheFlush(where, 4);
+        }
+
+    private:
+        static char const * nameGpReg(int reg)
+        {
+            ASSERT(reg <= 31);
+            ASSERT(reg >= 0);
+            static char const * names[] = {
+                "%g0", "%g1", "%g2", "%g3",
+                "%g4", "%g5", "%g6", "%g7",
+                "%o0", "%o1", "%o2", "%o3",
+                "%o4", "%o5", "%sp", "%o7",
+                "%l0", "%l1", "%l2", "%l3",
+                "%l4", "%l5", "%l6", "%l7",
+                "%i0", "%i1", "%i2", "%i3",
+                "%i4", "%i5", "%fp", "%i7"
+            };
+            return names[reg];
+        }
+
+        static char const * nameFpReg(int reg)
+        {
+            ASSERT(reg <= 31);
+            ASSERT(reg >= 0);
+            static char const * names[] = {
+                "%f0",   "%f1",   "%f2",   "%f3",
+                "%f4",   "%f5",   "%f6",   "%f7",
+                "%f8",   "%f9",  "%f10",  "%f11",
+                "%f12",  "%f13",  "%f14",  "%f15",
+                "%f16",  "%f17",  "%f18",  "%f19",
+                "%f20",  "%f21",  "%f22",  "%f23",
+                "%f24",  "%f25",  "%f26",  "%f27",
+                "%f28",  "%f29",  "%f30",  "%f31"
+            };
+            return names[reg];
+        }
+
+        static char const * nameICC(Condition cc)
+        {
+            ASSERT(cc <= ConditionVC);
+            ASSERT(cc >= 0);
+
+            uint32_t    ccIndex = cc;
+            static char const * inames[] = {
+                "   ", "e  ",
+                "le ", "l  ",
+                "leu", "cs ",
+                "neg", "vs ",
+                "a  ", "ne ",
+                "g  ", "ge ",
+                "gu ", "cc ",
+                "   ", "vc "
+            };
+            return inames[ccIndex];
+        }
+
+        static char const * nameFCC(DoubleCondition cc)
+        {
+            ASSERT(cc <= DoubleConditionULE);
+            ASSERT(cc >= 0);
+
+            uint32_t    ccIndex = cc;
+            static char const * fnames[] = {
+                "   ", "ne ",
+                "   ", "ul ",
+                "l  ", "ug ",
+                "g  ", "   ",
+                "   ", "e  ",
+                "ue ", "ge ",
+                "ugu", "le ",
+                "ule", "   "
+            };
+            return fnames[ccIndex];
+        }
+
+
+    };
+
+} // namespace JSC
+
+#endif // ENABLE(ASSEMBLER) && CPU(SPARC)
+
+#endif // SparcAssembler_h
diff --git a/js/src/assembler/jit/ExecutableAllocator.h b/js/src/assembler/jit/ExecutableAllocator.h
index 511aebf..50d58ef 100644
--- a/assembler/jit/ExecutableAllocator.h
+++ b/assembler/jit/ExecutableAllocator.h
@@ -35,6 +35,10 @@
 #include "jsvector.h"
 #include "jslock.h"
 
+#if WTF_CPU_SPARC
+extern  "C" void sync_instruction_memory(caddr_t v, u_int len);
+#endif
+
 #if WTF_PLATFORM_IPHONE
 #include <libkern/OSCacheControl.h>
 #include <sys/mman.h>
@@ -391,6 +395,11 @@ public:
     {
         CacheRangeFlush(code, size, CACHE_SYNC_ALL);
     }
+#elif WTF_CPU_SPARC
+    static void cacheFlush(void* code, size_t size)
+    {
+        sync_instruction_memory((caddr_t)code, size);
+    }
 #else
     #error "The cacheFlush support is missing on this platform."
 #endif
diff --git a/js/src/assembler/wtf/Platform.h b/js/src/assembler/wtf/Platform.h
index 3dfad2a..5ad3f75 100644
--- a/assembler/wtf/Platform.h
+++ b/assembler/wtf/Platform.h
@@ -159,7 +159,7 @@
 
 /* CPU(SPARC) - any SPARC, true for CPU(SPARC32) and CPU(SPARC64) */
 #if WTF_CPU_SPARC32 || WTF_CPU_SPARC64
-#define WTF_CPU_SPARC
+#define WTF_CPU_SPARC 1
 #endif
 
 /* CPU(X86) - i386 / x86 32-bit */
@@ -857,6 +857,8 @@ on MinGW. See https://bugs.webkit.org/show_bug.cgi?id=29268 */
 /* The JIT is tested & working on x86 Windows */
 #elif WTF_CPU_X86 && WTF_PLATFORM_WIN
     #define ENABLE_JIT 1
+#elif WTF_CPU_SPARC
+    #define ENABLE_JIT 1
 #endif
 
 #if WTF_PLATFORM_QT
@@ -918,8 +920,10 @@ on MinGW. See https://bugs.webkit.org/show_bug.cgi?id=29268 */
 #if !defined(ENABLE_YARR_JIT)
 
 /* YARR supports x86 & x86-64, and has been tested on Mac and Windows. */
+
 #if (WTF_CPU_X86 \
  || WTF_CPU_X86_64 \
+ || WTF_CPU_SPARC \
  || WTF_CPU_ARM_TRADITIONAL \
  || WTF_CPU_ARM_THUMB2 \
  || WTF_CPU_X86)
diff --git a/js/src/configure.in b/js/src/configure.in
index 5d5365b..faac6b5 100644
--- a/configure.in
+++ b/configure.in
@@ -2959,7 +2959,12 @@ arm*-*)
 sparc*-*)
     ENABLE_TRACEJIT=1
     NANOJIT_ARCH=Sparc
+    ENABLE_METHODJIT=1
+    ENABLE_MONOIC=1
+    ENABLE_POLYIC=1
+    ENABLE_POLYIC_TYPED_ARRAY=1
     AC_DEFINE(JS_CPU_SPARC)
+    AC_DEFINE(JS_NUNBOX32)
     ;;
 esac
 
diff --git a/js/src/jsgc.cpp b/js/src/jsgc.cpp
index 3fe567d..a912902 100644
--- a/jsgc.cpp
+++ b/jsgc.cpp
@@ -2799,7 +2799,7 @@ js_GC(JSContext *cx, JSCompartment *comp, JSGCInvocationKind gckind)
                  JS_CHECK_STACK_SIZE(cx->stackLimit + (1 << 14), &stackDummy));
 # else
     /* -16k because it is possible to perform a GC during an overrecursion report. */
-    JS_ASSERT_IF(cx->stackLimit, JS_CHECK_STACK_SIZE(cx->stackLimit - (1 << 14), &stackDummy));
+    JS_ASSERT_IF(cx->stackLimit, JS_CHECK_STACK_SIZE(cx->stackLimit - (1 << 15), &stackDummy));
 # endif
 #endif
 
diff --git a/js/src/methodjit/BaseAssembler.h b/js/src/methodjit/BaseAssembler.h
index bc7cd97..400e373 100644
--- a/methodjit/BaseAssembler.h
+++ b/methodjit/BaseAssembler.h
@@ -176,6 +176,8 @@ class Assembler : public ValueAssembler
     static const RegisterID ClobberInCall = JSC::X86Registers::ecx;
 #elif defined(JS_CPU_ARM)
     static const RegisterID ClobberInCall = JSC::ARMRegisters::r2;
+#elif defined(JS_CPU_SPARC)
+    static const RegisterID ClobberInCall = JSC::SparcRegisters::l1;
 #endif
 
     /* :TODO: OOM */
@@ -225,6 +227,10 @@ static const JSC::MacroAssembler::RegisterID JSParamReg_Argc   = JSC::X86Registe
 static const JSC::MacroAssembler::RegisterID JSReturnReg_Type  = JSC::ARMRegisters::r2;
 static const JSC::MacroAssembler::RegisterID JSReturnReg_Data  = JSC::ARMRegisters::r1;
 static const JSC::MacroAssembler::RegisterID JSParamReg_Argc   = JSC::ARMRegisters::r1;
+#elif defined(JS_CPU_SPARC)
+static const JSC::MacroAssembler::RegisterID JSReturnReg_Type = JSC::SparcRegisters::i0;
+static const JSC::MacroAssembler::RegisterID JSReturnReg_Data = JSC::SparcRegisters::i1;
+static const JSC::MacroAssembler::RegisterID JSParamReg_Argc  = JSC::SparcRegisters::i2;
 #endif
 
     size_t distanceOf(Label l) {
@@ -364,10 +370,18 @@ static const JSC::MacroAssembler::RegisterID JSParamReg_Argc   = JSC::ARMRegiste
     // Windows x64 requires extra space in between calls.
 #ifdef _WIN64
     static const uint32 ShadowStackSpace = 32;
+#elif defined(JS_CPU_SPARC)
+    static const uint32 ShadowStackSpace = 92;
 #else
     static const uint32 ShadowStackSpace = 0;
 #endif
 
+#if defined(JS_CPU_SPARC)
+    static const uint32 BaseStackSpace = 104;
+#else
+    static const uint32 BaseStackSpace = 0;
+#endif
+
     // Prepare the stack for a call sequence. This must be called AFTER all
     // volatile regs have been saved, and BEFORE pushArg() is used. The stack
     // is assumed to be aligned to 16-bytes plus any pushes that occured via
@@ -426,7 +440,7 @@ static const JSC::MacroAssembler::RegisterID JSParamReg_Argc   = JSC::ARMRegiste
         //
         // Note that it's not required we're in a call - stackAdjust can be 0.
         JS_ASSERT(marker.base <= extraStackSpace);
-        return Address(stackPointerRegister, stackAdjust + extraStackSpace - marker.base);
+        return Address(stackPointerRegister, BaseStackSpace + stackAdjust + extraStackSpace - marker.base);
     }
 
     // This is an internal function only for use inside a setupABICall(),
@@ -651,9 +665,9 @@ static const JSC::MacroAssembler::RegisterID JSParamReg_Argc   = JSC::ARMRegiste
         Address capacity(objReg, offsetof(JSObject, capacity));
         if (key.isConstant()) {
             JS_ASSERT(key.index() >= 0);
-            return branch32(BelowOrEqual, payloadOf(capacity), Imm32(key.index()));
+            return branch32(BelowOrEqual, capacity, Imm32(key.index()));
         }
-        return branch32(BelowOrEqual, payloadOf(capacity), key.reg());
+        return branch32(BelowOrEqual, capacity, key.reg());
     }
 
     // Load a jsval from an array slot, given a key. |objReg| is clobbered.
diff --git a/js/src/methodjit/Compiler.cpp b/js/src/methodjit/Compiler.cpp
index 5d77692..02c75e5 100644
--- a/methodjit/Compiler.cpp
+++ b/methodjit/Compiler.cpp
@@ -1845,7 +1845,7 @@ mjit::Compiler::generateMethod()
             // obj->getFlatClosureUpvars()
             masm.loadPtr(Address(reg, offsetof(JSObject, slots)), reg);
             Address upvarAddress(reg, JSObject::JSSLOT_FLAT_CLOSURE_UPVARS * sizeof(Value));
-            masm.loadPrivate(upvarAddress, reg);
+            masm.loadPrivate(masm.payloadOf(upvarAddress), reg);
             // push ((Value *) reg)[index]
             frame.freeReg(reg);
             frame.push(Address(reg, index * sizeof(Value)));
@@ -3605,7 +3605,7 @@ mjit::Compiler::jsop_bindname(JSAtom *atom, bool usePropCache)
     masm.loadPtr(Address(JSFrameReg, JSStackFrame::offsetOfScopeChain()), pic.objReg);
 
     pic.shapeGuard = masm.label();
-    Jump inlineJump = masm.branchPtr(Assembler::NotEqual, masm.payloadOf(parent), ImmPtr(0));
+    Jump inlineJump = masm.branchPtr(Assembler::NotEqual, parent, ImmPtr(0));
     {
         RESERVE_OOL_SPACE(stubcc.masm);
         pic.slowPathStart = stubcc.linkExit(inlineJump, Uses(0));
@@ -3674,7 +3674,7 @@ mjit::Compiler::jsop_bindname(JSAtom *atom, bool usePropCache)
 
     Address address(reg, offsetof(JSObject, parent));
 
-    Jump j = masm.branchPtr(Assembler::NotEqual, masm.payloadOf(address), ImmPtr(0));
+    Jump j = masm.branchPtr(Assembler::NotEqual, address, ImmPtr(0));
 
     stubcc.linkExit(j, Uses(0));
     stubcc.leave();
@@ -4614,7 +4614,7 @@ mjit::Compiler::jsop_instanceof()
     Label loop = masm.label();
 
     /* Walk prototype chain, break out on NULL or hit. */
-    masm.loadPayload(protoAddr, obj);
+    masm.loadPtr(protoAddr, obj);
     Jump isFalse2 = masm.branchTestPtr(Assembler::Zero, obj, obj);
     Jump isTrue = masm.branchPtr(Assembler::NotEqual, obj, proto);
     isTrue.linkTo(loop, &masm);
diff --git a/js/src/methodjit/FastArithmetic.cpp b/js/src/methodjit/FastArithmetic.cpp
index 653581d..3eb151b 100644
--- a/methodjit/FastArithmetic.cpp
+++ b/methodjit/FastArithmetic.cpp
@@ -771,7 +771,7 @@ mjit::Compiler::jsop_neg()
 #if defined JS_CPU_X86 || defined JS_CPU_X64
         masm.loadDouble(&DoubleNegMask, FPRegisters::Second);
         masm.xorDouble(FPRegisters::Second, fpreg);
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         masm.negDouble(fpreg, fpreg);
 #endif
 
diff --git a/js/src/methodjit/FastOps.cpp b/js/src/methodjit/FastOps.cpp
index 7e1c5b1..3227e30 100644
--- a/methodjit/FastOps.cpp
+++ b/methodjit/FastOps.cpp
@@ -1556,13 +1556,13 @@ mjit::Compiler::jsop_stricteq(JSOp op)
 
         Assembler::Condition oppositeCond = (op == JSOP_STRICTEQ) ? Assembler::NotEqual : Assembler::Equal;
 
-#if defined JS_CPU_X86 || defined JS_CPU_ARM
+#ifndef JS_CPU_X64
         static const int CanonicalNaNType = 0x7FF80000;
         masm.setPtr(oppositeCond, treg, Imm32(CanonicalNaNType), result);
-#elif defined JS_CPU_X64
+#else
         static const void *CanonicalNaNType = (void *)0x7FF8000000000000; 
-        masm.move(ImmPtr(CanonicalNaNType), JSC::X86Registers::r11);
-        masm.setPtr(oppositeCond, treg, JSC::X86Registers::r11, result);
+        masm.move(ImmPtr(CanonicalNaNType), Registers::ScratchReg);
+        masm.setPtr(oppositeCond, treg, Registers::ScratchReg, result);
 #endif
 
         frame.popn(2);
@@ -1585,13 +1585,13 @@ mjit::Compiler::jsop_stricteq(JSOp op)
 
         /* This is only true if the other side is |null|. */
         RegisterID result = frame.allocReg(Registers::SingleByteRegs);
-#if defined JS_CPU_X86 || defined JS_CPU_ARM
+#ifndef JS_CPU_X64
         JSValueTag mask = known->getKnownTag();
         if (frame.shouldAvoidTypeRemat(test))
             masm.set32(cond, masm.tagOf(frame.addressOf(test)), Imm32(mask), result);
         else
             masm.set32(cond, frame.tempRegForType(test), Imm32(mask), result);
-#elif defined JS_CPU_X64
+#else
         RegisterID maskReg = frame.allocReg();
         masm.move(ImmTag(known->getKnownTag()), maskReg);
 
@@ -1667,7 +1667,7 @@ mjit::Compiler::jsop_stricteq(JSOp op)
         return;
     }
 
-#ifndef JS_CPU_ARM
+#if !defined JS_CPU_ARM && !defined JS_CPU_SPARC
     /* Try an integer fast-path. */
     bool needStub = false;
     if (!lhs->isTypeKnown()) {
diff --git a/js/src/methodjit/FrameState-inl.h b/js/src/methodjit/FrameState-inl.h
index 9dfeb82..7764a3f 100644
--- a/methodjit/FrameState-inl.h
+++ b/methodjit/FrameState-inl.h
@@ -137,7 +137,7 @@ FrameState::convertInt32ToDouble(Assembler &masm, FrameEntry *fe, FPRegisterID f
     if (fe->data.inRegister())
         masm.convertInt32ToDouble(fe->data.reg(), fpreg);
     else
-        masm.convertInt32ToDouble(addressOf(fe), fpreg);
+        masm.convertInt32ToDouble(masm.payloadOf(addressOf(fe)), fpreg);
 }
 
 inline bool
diff --git a/js/src/methodjit/ICLabels.h b/js/src/methodjit/ICLabels.h
index cddcf7a..98fdffd 100644
--- a/methodjit/ICLabels.h
+++ b/methodjit/ICLabels.h
@@ -64,7 +64,7 @@ namespace ic {
  * implementation.
  */
 
-#if defined JS_CPU_X64 || defined JS_CPU_ARM
+#if defined JS_CPU_X64 || defined JS_CPU_ARM || defined JS_CPU_SPARC
 # define JS_HAS_IC_LABELS
 #endif
 
@@ -172,7 +172,7 @@ struct GetPropLabels : MacroAssemblerTypedefs {
     int getInlineTypeJumpOffset() {
 #if defined JS_CPU_X86 || defined JS_CPU_X64
         return INLINE_TYPE_JUMP;
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         return POST_INST_OFFSET(inlineTypeJumpOffset);
 #endif
     }
@@ -180,7 +180,7 @@ struct GetPropLabels : MacroAssemblerTypedefs {
     void setInlineTypeJumpOffset(int offset) {
 #if defined JS_CPU_X86 || defined JS_CPU_X64
         JS_ASSERT(INLINE_TYPE_JUMP == offset);
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         inlineTypeJumpOffset = offset;
         JS_ASSERT(offset == inlineTypeJumpOffset);
 #endif
@@ -225,6 +225,10 @@ struct GetPropLabels : MacroAssemblerTypedefs {
 
     /* Offset from the fast path to the type guard jump. */
     int32 inlineTypeJumpOffset : 8;
+#elif defined JS_CPU_SPARC
+    static const int32 INLINE_SHAPE_JUMP = 48;
+    static const int32 INLINE_TYPE_JUMP = 48;
+    int32 inlineTypeJumpOffset : 8;
 #endif
 };
 
diff --git a/js/src/methodjit/ICRepatcher.h b/js/src/methodjit/ICRepatcher.h
index 59d8e78..24b08b9 100644
--- a/methodjit/ICRepatcher.h
+++ b/methodjit/ICRepatcher.h
@@ -72,7 +72,7 @@ class Repatcher : public JSC::RepatchBuffer
 
     /* Patch a stub call. */
     void relink(CodeLocationCall call, FunctionPtr stub) {
-#if defined JS_CPU_X64 || defined JS_CPU_X86
+#if defined JS_CPU_X64 || defined JS_CPU_X86 || defined JS_CPU_SPARC
         JSC::RepatchBuffer::relink(call, stub);
 #elif defined JS_CPU_ARM
         /*
@@ -95,7 +95,7 @@ class Repatcher : public JSC::RepatchBuffer
 
     /* Patch the offset of a Value load emitted by loadValueWithAddressOffsetPatch. */
     void patchAddressOffsetForValueLoad(CodeLocationLabel label, uint32 offset) {
-#if defined JS_CPU_X64 || defined JS_CPU_ARM
+#if defined JS_CPU_X64 || defined JS_CPU_ARM || defined JS_CPU_SPARC
         repatch(label.dataLabel32AtOffset(0), offset);
 #elif defined JS_CPU_X86
         static const unsigned LOAD_TYPE_OFFSET = 6;
@@ -115,7 +115,7 @@ class Repatcher : public JSC::RepatchBuffer
     }
 
     void patchAddressOffsetForValueStore(CodeLocationLabel label, uint32 offset, bool typeConst) {
-#if defined JS_CPU_ARM || defined JS_CPU_X64
+#if defined JS_CPU_ARM || defined JS_CPU_X64 || defined JS_CPU_SPARC
         (void) typeConst;
         repatch(label.dataLabel32AtOffset(0), offset);
 #elif defined JS_CPU_X86
diff --git a/js/src/methodjit/MachineRegs.h b/js/src/methodjit/MachineRegs.h
index 149f188..c786cd3 100644
--- a/methodjit/MachineRegs.h
+++ b/methodjit/MachineRegs.h
@@ -60,6 +60,7 @@ struct Registers {
     static const RegisterID TypeMaskReg = JSC::X86Registers::r13;
     static const RegisterID PayloadMaskReg = JSC::X86Registers::r14;
     static const RegisterID ValueReg = JSC::X86Registers::r10;
+    static const RegisterID ScratchReg = JSC::X86Registers::r11;
 #endif
 
     // Register that homes the current JSStackFrame.
@@ -67,6 +68,8 @@ struct Registers {
     static const RegisterID JSFrameReg = JSC::X86Registers::ebx;
 #elif defined(JS_CPU_ARM)
     static const RegisterID JSFrameReg = JSC::ARMRegisters::r11;
+#elif defined(JS_CPU_SPARC)
+    static const RegisterID JSFrameReg = JSC::SparcRegisters::l0;
 #endif
 
 #if defined(JS_CPU_X86) || defined(JS_CPU_X64)
@@ -87,6 +90,14 @@ struct Registers {
     static const RegisterID ArgReg0 = JSC::ARMRegisters::r0;
     static const RegisterID ArgReg1 = JSC::ARMRegisters::r1;
     static const RegisterID ArgReg2 = JSC::ARMRegisters::r2;
+#elif JS_CPU_SPARC
+    static const RegisterID ReturnReg = JSC::SparcRegisters::o0;
+    static const RegisterID ArgReg0 = JSC::SparcRegisters::o0;
+    static const RegisterID ArgReg1 = JSC::SparcRegisters::o1;
+    static const RegisterID ArgReg2 = JSC::SparcRegisters::o2;
+    static const RegisterID ArgReg3 = JSC::SparcRegisters::o3;
+    static const RegisterID ArgReg4 = JSC::SparcRegisters::o4;
+    static const RegisterID ArgReg5 = JSC::SparcRegisters::o5;
 #endif
 
     static const RegisterID StackPointer = JSC::MacroAssembler::stackPointerRegister;
@@ -168,6 +179,30 @@ struct Registers {
     // r15 is PC (program counter).
 
     static const uint32 SingleByteRegs = TempRegs | SavedRegs;
+#elif defined(JS_CPU_SPARC)
+    static const uint32 TempRegs =
+          (1 << JSC::SparcRegisters::o0)
+        | (1 << JSC::SparcRegisters::o1)
+        | (1 << JSC::SparcRegisters::o2)
+        | (1 << JSC::SparcRegisters::o3)
+        | (1 << JSC::SparcRegisters::o4)
+        | (1 << JSC::SparcRegisters::o5);
+
+    static const uint32 SavedRegs =
+          (1 << JSC::SparcRegisters::l2)
+        | (1 << JSC::SparcRegisters::l3)
+        | (1 << JSC::SparcRegisters::l4)
+        | (1 << JSC::SparcRegisters::l5)
+        | (1 << JSC::SparcRegisters::l6)
+        | (1 << JSC::SparcRegisters::l7)
+        | (1 << JSC::SparcRegisters::i0)
+        | (1 << JSC::SparcRegisters::i1)
+        | (1 << JSC::SparcRegisters::i2)
+        | (1 << JSC::SparcRegisters::i3)
+        | (1 << JSC::SparcRegisters::i4)
+        | (1 << JSC::SparcRegisters::i5);
+
+    static const uint32 SingleByteRegs = TempRegs | SavedRegs;
 #else
 # error "Unsupported platform"
 #endif
@@ -195,6 +230,8 @@ struct Registers {
 # endif
 #elif defined(JS_CPU_ARM)
         return 4;
+#elif defined(JS_CPU_SPARC)
+        return 6;
 #endif
     }
 
@@ -236,6 +273,15 @@ struct Registers {
             JSC::ARMRegisters::r2,
             JSC::ARMRegisters::r3
         };
+#elif defined(JS_CPU_SPARC)
+        static const RegisterID regs[] = {
+            JSC::SparcRegisters::o0,
+            JSC::SparcRegisters::o1,
+            JSC::SparcRegisters::o2,
+            JSC::SparcRegisters::o3,
+            JSC::SparcRegisters::o4,
+            JSC::SparcRegisters::o5
+        };
 #endif
         JS_ASSERT(numArgRegs(conv) == JS_ARRAY_LENGTH(regs));
         if (i > JS_ARRAY_LENGTH(regs))
@@ -362,6 +408,27 @@ struct FPRegisters {
     static const FPRegisterID Second = JSC::ARMRegisters::d1;
     static const FPRegisterID Temp0 = JSC::ARMRegisters::d2;
     static const FPRegisterID Temp1 = JSC::ARMRegisters::d3;
+#elif defined(JS_CPU_SPARC)
+    static const uint32 TotalFPRegisters = 16;
+    static const uint32 TempFPRegs = 
+          (1 << JSC::SparcRegisters::f0)
+        | (1 << JSC::SparcRegisters::f2)
+        | (1 << JSC::SparcRegisters::f4)
+        | (1 << JSC::SparcRegisters::f6)
+        | (1 << JSC::SparcRegisters::f8)
+        | (1 << JSC::SparcRegisters::f10)
+        | (1 << JSC::SparcRegisters::f12)
+        | (1 << JSC::SparcRegisters::f14)
+        | (1 << JSC::SparcRegisters::f16)
+        | (1 << JSC::SparcRegisters::f18)
+        | (1 << JSC::SparcRegisters::f20)
+        | (1 << JSC::SparcRegisters::f22)
+        | (1 << JSC::SparcRegisters::f24)
+        | (1 << JSC::SparcRegisters::f26)
+        | (1 << JSC::SparcRegisters::f28);
+    /* FIXME: Temporary hack until FPRegister allocation exists. */
+    static const FPRegisterID First  = JSC::SparcRegisters::f0;
+    static const FPRegisterID Second = JSC::SparcRegisters::f2;
 #else
 # error "Unsupported platform"
 #endif
diff --git a/js/src/methodjit/MethodJIT.cpp b/js/src/methodjit/MethodJIT.cpp
index 6169972..089a3be 100644
--- a/methodjit/MethodJIT.cpp
+++ b/methodjit/MethodJIT.cpp
@@ -555,6 +555,7 @@ SYMBOL_STRING(JaegerStubVeneer) ":"         "\n"
 "   pop     {ip,pc}"                        "\n"
 );
 
+# elif defined(JS_CPU_SPARC)
 # else
 #  error "Unsupported CPU!"
 # endif
diff --git a/js/src/methodjit/MethodJIT.h b/js/src/methodjit/MethodJIT.h
index 8c2ab39..9529486 100644
--- a/methodjit/MethodJIT.h
+++ b/methodjit/MethodJIT.h
@@ -45,6 +45,7 @@
 
 #if !defined JS_CPU_X64 && \
     !defined JS_CPU_X86 && \
+    !defined JS_CPU_SPARC && \
     !defined JS_CPU_ARM
 # error "Oh no, you should define a platform so this compiles."
 #endif
@@ -59,6 +60,39 @@ namespace mjit { struct JITScript; }
 
 struct VMFrame
 {
+#if defined(JS_CPU_SPARC)
+    void *savedL0;
+    void *savedL1;
+    void *savedL2;
+    void *savedL3;
+    void *savedL4;
+    void *savedL5;
+    void *savedL6;
+    void *savedL7;
+    void *savedI0;
+    void *savedI1;
+    void *savedI2;
+    void *savedI3;
+    void *savedI4;
+    void *savedI5;
+    void *savedI6;
+    void *savedI7;
+
+    void *str_p;
+
+    void *outgoing_p0;
+    void *outgoing_p1;
+    void *outgoing_p2;
+    void *outgoing_p3;
+    void *outgoing_p4;
+    void *outgoing_p5;
+
+    void *outgoing_p6;
+
+    void *reserve_0;
+    void *reserve_1;
+#endif
+
     union Arguments {
         struct {
             void *ptr;
@@ -131,6 +165,13 @@ struct VMFrame
     inline void** returnAddressLocation() {
         return reinterpret_cast<void**>(this) - 1;
     }
+#elif defined(JS_CPU_SPARC)
+    JSStackFrame *topRetrunAddr;
+    void* veneerReturn;
+    void* _align;
+    inline void** returnAddressLocation() {
+        return reinterpret_cast<void**>(&this->veneerReturn);
+    }
 #else
 # error "The VMFrame layout isn't defined for your processor architecture!"
 #endif
diff --git a/js/src/methodjit/MonoIC.h b/js/src/methodjit/MonoIC.h
index 66254b1..ec33db7 100644
--- a/methodjit/MonoIC.h
+++ b/methodjit/MonoIC.h
@@ -125,15 +125,15 @@ struct SetGlobalNameIC : public GlobalNameIC
     JSC::JITCode            extraStub;
 
     /* SET only, if we had to generate an out-of-line path. */
-    int inlineShapeJump : 10;   /* Offset into inline path for shape jump. */
-    int extraShapeGuard : 6;    /* Offset into stub for shape guard. */
+    int32 inlineShapeJump : 10;   /* Offset into inline path for shape jump. */
+    int32 extraShapeGuard : 6;    /* Offset into stub for shape guard. */
     bool objConst : 1;          /* True if the object is constant. */
     RegisterID objReg   : 5;    /* Register for object, if objConst is false. */
     RegisterID shapeReg : 5;    /* Register for shape; volatile. */
     bool hasExtraStub : 1;      /* Extra stub is preset. */
 
-    int fastRejoinOffset : 16;  /* Offset from fastPathStart to rejoin. */
-    int extraStoreOffset : 16;  /* Offset into store code. */
+    int32 fastRejoinOffset : 16;  /* Offset from fastPathStart to rejoin. */
+    int32 extraStoreOffset : 16;  /* Offset into store code. */
 
     /* SET only. */
     ValueRemat vr;              /* RHS value. */
diff --git a/js/src/methodjit/NunboxAssembler.h b/js/src/methodjit/NunboxAssembler.h
index 6ce0ac8..fcce552 100644
--- a/methodjit/NunboxAssembler.h
+++ b/methodjit/NunboxAssembler.h
@@ -72,16 +72,24 @@ struct ImmPayload : JSC::MacroAssembler::Imm32
 
 class NunboxAssembler : public JSC::MacroAssembler
 {
+  public:
+#ifdef IS_BIG_ENDIAN
+    static const uint32 PAYLOAD_OFFSET = 4;
+    static const uint32 TAG_OFFSET     = 0;
+#else
     static const uint32 PAYLOAD_OFFSET = 0;
     static const uint32 TAG_OFFSET     = 4;
+#endif
 
   public:
     static const JSC::MacroAssembler::Scale JSVAL_SCALE = JSC::MacroAssembler::TimesEight;
 
-    template <typename T>
-    T payloadOf(T address) {
-        JS_ASSERT(PAYLOAD_OFFSET == 0);
-        return address;
+    Address payloadOf(Address address) {
+        return Address(address.base, address.offset + PAYLOAD_OFFSET);
+    }
+  
+    BaseIndex payloadOf(BaseIndex address) {
+        return BaseIndex(address.base, address.index, address.scale, address.offset + PAYLOAD_OFFSET);
     }
 
     Address tagOf(Address address) {
@@ -185,7 +193,7 @@ class NunboxAssembler : public JSC::MacroAssembler
         JS_ASSERT(differenceBetween(start, endType) == 6);
         JS_ASSERT(differenceBetween(endType, endPayload) == 6);
         return start;
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         /* 
          * On ARM, the first instruction loads the offset from a literal pool, so the label
          * returned points at that instruction.
@@ -217,7 +225,7 @@ class NunboxAssembler : public JSC::MacroAssembler
         JS_ASSERT(differenceBetween(start, endType) == 6);
         JS_ASSERT(differenceBetween(endType, endPayload) == 6);
         return start;
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         return store64WithAddressOffsetPatch(treg, dreg, address);
 #endif
     }
@@ -233,7 +241,7 @@ class NunboxAssembler : public JSC::MacroAssembler
         JS_ASSERT(differenceBetween(start, endType) == 10);
         JS_ASSERT(differenceBetween(endType, endPayload) == 6);
         return start;
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         return store64WithAddressOffsetPatch(type, dreg, address);
 #endif
     }
@@ -253,7 +261,7 @@ class NunboxAssembler : public JSC::MacroAssembler
         JS_ASSERT(differenceBetween(start, endType) == 10);
         JS_ASSERT(differenceBetween(endType, endPayload) == 10);
         return start;
-#elif defined JS_CPU_ARM
+#elif defined JS_CPU_ARM || defined JS_CPU_SPARC
         return store64WithAddressOffsetPatch(type, payload, address);
 #endif
     }
@@ -435,6 +443,8 @@ class NunboxAssembler : public JSC::MacroAssembler
         m_assembler.movd_rr(srcDest, dataReg);
         m_assembler.psrldq_rr(srcDest, 4);
         m_assembler.movd_rr(srcDest, typeReg);
+#elif defined JS_CPU_SPARC
+        breakDoubleTo32(srcDest, typeReg, dataReg);
 #else
         JS_NOT_REACHED("implement this - push double, pop pop is easiest");
 #endif
diff --git a/js/src/methodjit/PolyIC.cpp b/js/src/methodjit/PolyIC.cpp
index aa82dab..dc7f3da 100644
--- a/methodjit/PolyIC.cpp
+++ b/methodjit/PolyIC.cpp
@@ -431,7 +431,7 @@ class SetPropCompiler : public PICStubCompiler
             for (Jump *pj = otherGuards.begin(); pj != otherGuards.end(); ++pj)
                 pj->linkTo(masm.label(), &masm);
             slowExit = masm.jump();
-            pic.secondShapeGuard = masm.distanceOf(masm.label()) - masm.distanceOf(start);
+            pic.secondShapeGuard = masm.differenceBetween(start, slowExit.get());
         } else {
             pic.secondShapeGuard = 0;
         }
@@ -1075,7 +1075,7 @@ class GetPropCompiler : public PICStubCompiler
             if (!shapeMismatches.append(j))
                 return error();
 
-            pic.secondShapeGuard = masm.distanceOf(masm.label()) - masm.distanceOf(start);
+            pic.secondShapeGuard = masm.differenceBetween(start, j);
         } else {
             pic.secondShapeGuard = 0;
         }
diff --git a/js/src/methodjit/PolyIC.h b/js/src/methodjit/PolyIC.h
index 14e53e3..f6a025f 100644
--- a/methodjit/PolyIC.h
+++ b/methodjit/PolyIC.h
@@ -94,7 +94,7 @@ struct BaseIC : public MacroAssemblerTypedefs {
     // Offset from start of stub to jump target of second shape guard as Nitro
     // asm data location. This is 0 if there is only one shape guard in the
     // last stub.
-    int secondShapeGuard : 11;
+    int32 secondShapeGuard : 11;
 
     // Opcode this was compiled for.
     JSOp op : 9;
@@ -255,9 +255,9 @@ struct GetElementIC : public BasePolyIC {
     // These offsets are used for string-key dependent stubs, such as named
     // property accesses. They are separated from the int-key dependent stubs,
     // in order to guarantee that the id type needs only one guard per type.
-    int atomGuard : 8;          // optional, non-zero if present
-    int firstShapeGuard : 8;    // always set
-    int secondShapeGuard : 8;   // optional, non-zero if present
+    int32 atomGuard : 8;          // optional, non-zero if present
+    int32 firstShapeGuard : 11;    // always set
+    int32 secondShapeGuard : 11;   // optional, non-zero if present
 
     bool hasLastStringStub : 1;
     JITCode lastStringStub;
@@ -336,7 +336,7 @@ struct SetElementIC : public BaseIC {
 
     // A bitmask of registers that are volatile and must be preserved across
     // stub calls inside the IC.
-    uint32 volatileMask : 16;
+    uint32 volatileMask;
 
     // If true, then keyValue contains a constant index value >= 0. Otherwise,
     // keyReg contains a dynamic integer index in any range.
diff --git a/js/src/methodjit/TrampolineSparc.s b/js/src/methodjit/TrampolineSparc.s
new file mode 100644
index 0000000..7fb929f
--- /dev/null
+++ b/methodjit/TrampolineSparc.s
@@ -0,0 +1,117 @@
+! -*- Mode: C++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
+! ***** BEGIN LICENSE BLOCK *****
+! Version: MPL 1.1!GPL 2.0!LGPL 2.1
+!
+! The contents of this file are subject to the Mozilla Public License Version
+! 1.1 (the "License")! you may not use this file except in compliance with
+! the License. You may obtain a copy of the License at
+! http:!!www.mozilla.org!MPL!
+!
+! Software distributed under the License is distributed on an "AS IS" basis,
+! WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
+! for the specific language governing rights and limitations under the
+! License.
+!
+! The Original Code is Mozilla SpiderMonkey JavaScript 1.9 code, released
+! May 28, 2008.
+! 
+! The Initial Developer of the Original Code is
+!    Leon Sha <leon.sha@oracle.com>
+!
+! Portions created by the Initial Developer are Copyright (C) 2010-2011
+! the Initial Developer. All Rights Reserved.
+!
+! Contributor(s):
+!
+! Alternatively, the contents of this file may be used under the terms of
+! either the GNU General Public License Version 2 or later (the "GPL"), or
+! the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
+! in which case the provisions of the GPL or the LGPL are applicable instead
+! of those above. If you wish to allow use of your version of this file only
+! under the terms of either the GPL or the LGPL, and not to allow others to
+! use your version of this file under the terms of the MPL, indicate your
+! decision by deleting the provisions above and replace them with the notice
+! and other provisions required by the GPL or the LGPL. If you do not delete
+! the provisions above, a recipient may use your version of this file under
+! the terms of any one of the MPL, the GPL or the LGPL.
+!
+! ***** END LICENSE BLOCK *****
+
+.text
+
+! JSBool JaegerTrampoline(JSContext *cx, JSStackFrame *fp, void *code,
+!                        , uintptr_t inlineCallCount)
+.global JaegerTrampoline
+.type   JaegerTrampoline, #function
+JaegerTrampoline:
+    save    %sp,-160,%sp
+    st      %i3, [%fp - 20]         ! stackLimit
+    st      %i0, [%fp - 24]        ! cx
+    st      %i1, [%fp - 16]         ! entryFp
+    st      %i1, [%fp - 28]        ! regs->fp
+    mov     %i1, %l0               ! fp
+    call    SetVMFrameRegs
+    mov     %sp, %o0
+    call    PushActiveVMFrame
+    mov     %sp, %o0
+    jmp     %i2
+    st      %i7, [%fp - 12]         ! return address
+.size   JaegerTrampoline, . - JaegerTrampoline
+
+! void JaegerTrampolineReturn()
+.global JaegerTrampolineReturn
+.type   JaegerTrampolineReturn, #function
+JaegerTrampolineReturn:
+    st      %i0, [%l0 + 0x18]                        /* fp->rval type */
+    st      %i1, [%l0 + 0x1c]                        /* fp->rval data */
+    call    PopActiveVMFrame
+    mov     %sp, %o0
+    ld      [%fp - 12], %i7         ! return address
+    mov     1, %i0
+    ret
+    restore		
+.size   JaegerTrampolineReturn, . - JaegerTrampolineReturn
+
+! void *JaegerThrowpoline(js::VMFrame *vmFrame)
+.global JaegerThrowpoline
+.type   JaegerThrowpoline, #function
+JaegerThrowpoline:
+    call    js_InternalThrow
+    mov     %sp,%o0
+    tst     %o0
+    be      throwpoline_exit
+    nop
+    jmp     %o0
+    nop
+throwpoline_exit:
+    ta      3
+    mov     %sp, %o2
+    mov     %fp, %o3
+    ldd     [%o2 + (0*8)], %l0
+    ldd     [%o2 + (1*8)], %l2
+    ldd     [%o2 + (2*8)], %l4
+    ldd     [%o2 + (3*8)], %l6
+    ldd     [%o2 + (4*8)], %i0
+    ldd     [%o2 + (5*8)], %i2
+    ldd     [%o2 + (6*8)], %i4
+    ldd     [%o2 + (7*8)], %i6
+    ld      [%o3 - 12], %i7         ! return address
+    mov     %o2, %sp
+    call    PopActiveVMFrame
+    mov     %sp, %o0
+    clr     %i0
+    ret
+    restore
+.size   JaegerThrowpoline, . - JaegerThrowpoline
+
+.global InjectJaegerReturn
+.type   InjectJaegerReturn, #function
+InjectJaegerReturn:
+    ld      [%l0 + 0x18], %i0                        /* fp->rval type */
+    ld      [%l0 + 0x1c], %i1                        /* fp->rval data */
+    ld      [%l0 + 0x14], %i7                        /* fp->ncode */
+    sub     %i7, 8, %i7
+    ld      [%fp - 28], %l0
+    ret
+    nop
+.size   InjectJaegerReturn, . - InjectJaegerReturn
diff --git a/js/src/methodjit/TypedArrayIC.h b/js/src/methodjit/TypedArrayIC.h
index 88f8175..f97e6b0 100644
--- a/methodjit/TypedArrayIC.h
+++ b/methodjit/TypedArrayIC.h
@@ -46,7 +46,7 @@
 namespace js {
 namespace mjit {
 
-#if defined(JS_POLYIC) && (defined JS_CPU_X86 || defined JS_CPU_X64)
+#if defined(JS_POLYIC_TYPED_ARRAY)
 
 typedef JSC::MacroAssembler::RegisterID RegisterID;
 typedef JSC::MacroAssembler::FPRegisterID FPRegisterID;
diff --git a/js/src/nanojit/NativeSparc.cpp b/js/src/nanojit/NativeSparc.cpp
index 029a22a..a73be3d 100644
--- a/nanojit/NativeSparc.cpp
+++ b/nanojit/NativeSparc.cpp
@@ -46,9 +46,33 @@
 namespace nanojit
 {
 #ifdef FEATURE_NANOJIT
+    class SafeUnderrunProtect
+    {
+        public:
+            SafeUnderrunProtect(Assembler* assembler, int n)
+            {
+                _nprotect = n;
+                _assembler = assembler;
+                _assembler->underrunProtect(n);
+                _priorIns = _assembler->_nIns;
+                _priorStart = _assembler->codeStart;
+            };
+
+            ~SafeUnderrunProtect()
+            {
+                NanoAssert(_priorStart == _assembler->codeStart);
+                NanoAssert((intptr_t)_priorIns - (intptr_t)_assembler->_nIns <= _nprotect);
+            };
+
+        private:
+            int _nprotect;
+            Assembler* _assembler;
+            NIns* _priorIns;
+            NIns* _priorStart;
+    };
 
 #ifdef NJ_VERBOSE
-    const char *regNames[] = {
+    const char* regNames[] = {
         "%g0", "%g1", "%g2", "%g3", "%g4", "%g5", "%g6", "%g7",
         "%o0", "%o1", "%o2", "%o3", "%o4", "%o5", "%sp", "%o7",
         "%l0", "%l1", "%l2", "%l3", "%l4", "%l5", "%l6", "%l7",
@@ -78,13 +102,13 @@ namespace nanojit
     }
 
     inline void Assembler::IntegerOperation
-        (Register rs1, Register rs2, Register rd, int32_t op3, const char *opcode) {
+        (Register rs1, Register rs2, Register rd, int32_t op3, const char* opcode) {
         Format_3_1(2, rd, op3, rs1, 0, rs2);
         asm_output("%s %s, %s, %s", opcode, gpn(rs1), gpn(rs2), gpn(rd));
     }
 
     inline void Assembler::IntegerOperationI
-        (Register rs1, int32_t simm13, Register rd, int32_t op3, const char *opcode) {
+        (Register rs1, int32_t simm13, Register rd, int32_t op3, const char* opcode) {
         Format_3_1I(2, rd, op3, rs1, simm13);
         asm_output("%s %s, %d, %s", opcode, gpn(rs1), simm13, gpn(rd));
     }
@@ -126,9 +150,9 @@ namespace nanojit
         IntegerOperation(rs1, rs2, rd, 0x3, "xor");
     }
 
-    inline void Assembler::Bicc(int32_t a, int32_t dsp22, int32_t cond, const char *opcode) {
+    inline void Assembler::Bicc(int32_t a, int32_t dsp22, int32_t cond, const char* opcode) {
         Format_2_2(a, cond, 0x2, dsp22);
-        asm_output("%s 0x%x", opcode, _nIns + dsp22 - 1);
+        asm_output("%s 0x%x", opcode, _nIns + dsp22);
     }
 
     inline void Assembler::BA  (int32_t a, int32_t dsp22) { Bicc(a, dsp22, 0x8, "ba");   }
@@ -155,7 +179,7 @@ namespace nanojit
         asm_output("faddd %s, %s, %s", gpn(rs1), gpn(rs2), gpn(rd));
     }
 
-    inline void Assembler::FBfcc(int32_t a, int32_t dsp22, int32_t cond, const char *opcode) {
+    inline void Assembler::FBfcc(int32_t a, int32_t dsp22, int32_t cond, const char* opcode) {
         Format_2_2(a, cond, 0x6, dsp22);
         asm_output("%s 0x%x", opcode, _nIns + dsp22 - 1);
     }
@@ -178,7 +202,7 @@ namespace nanojit
     }
 
     inline void Assembler::FloatOperation
-        (Register rs1, Register rs2, Register rd, int32_t opf, const char *opcode) {
+        (Register rs1, Register rs2, Register rd, int32_t opf, const char* opcode) {
         Format_3_8(2, rd, 0x34, rs1, opf, rs2);
         if (rs1 != G0) {
           asm_output("%s %s, %s, %s", opcode, gpn(rs1), gpn(rs2), gpn(rd));
@@ -346,13 +370,13 @@ namespace nanojit
     }
 
     inline void Assembler::MOVcc
-        (Register rs, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char *opcode) {
+        (Register rs, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char* opcode) {
         Format_4_2(rd, 0x2c, cc2, cond, cc1, cc0, rs);
         asm_output("%s %s, %s", opcode, gpn(rs), gpn(rd));
     }
 
     inline void Assembler::MOVccI
-        (int32_t simm11, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char *opcode) {
+        (int32_t simm11, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char* opcode) {
         Format_4_2I(rd, 0x2c, cc2, cond, cc1, cc0, simm11);
         asm_output("%s 0x%x, %s", opcode, simm11, gpn(rd));
     }
@@ -385,7 +409,7 @@ namespace nanojit
     inline void Assembler::MOVFGI (int32_t simm11, Register rd) { MOVccI(simm11, 0, 0, 0, rd, 0x6, "movfg");  }
     inline void Assembler::MOVFGEI(int32_t simm11, Register rd) { MOVccI(simm11, 0, 0, 0, rd, 0xb, "movfge"); }
 
-    inline void Assembler::FMOVDcc(Register rs, int32_t opt_cc, Register rd, int32_t cond, const char *opcode) {
+    inline void Assembler::FMOVDcc(Register rs, int32_t opt_cc, Register rd, int32_t cond, const char* opcode) {
         Format_4_5(rd, 0x35, cond, opt_cc, 0x2, rs);
         asm_output("%s %s, %s", opcode, gpn(rs), gpn(rd));
     }
@@ -555,24 +579,31 @@ namespace nanojit
     }
 
     // general Assemble
-    inline void Assembler::JMP_long_nocheck(int32_t t) {
+    inline void Assembler::JMP_long(int32_t t) {
+        SafeUnderrunProtect protect(this, 16);
+    /*    if (t) {
+            int32_t tt = ((intptr_t)t - (intptr_t)_nIns + 16) >> 2;
+            if (isIMM22(tt)) {
+                NOP();
+                NOP();
+                NOP();
+                BA(0,tt);
+                return;
+            }
+        }
+*/
         NOP();
         JMPL(G0, G2, G0);
         ORI(G2, t & 0x3FF, G2);
         SETHI(t, G2);
     }
 
-    inline void Assembler::JMP_long(int32_t t) {
-        underrunProtect(16);
-        JMP_long_nocheck(t);
-    }
-
     inline void Assembler::JMP_long_placeholder() {
         JMP_long(0);
     }
 
-    inline int32_t Assembler::JCC(void *t) {
-        underrunProtect(32);
+    inline int32_t Assembler::JCC(void* t) {
+        SafeUnderrunProtect protect(this, 32);
         int32_t tt = ((intptr_t)t - (intptr_t)_nIns + 8) >> 2;
         if( !(isIMM22(tt)) ) {
             NOP();
@@ -586,7 +617,7 @@ namespace nanojit
         return tt;
     }
 
-    void Assembler::JMP(void *t) {
+    void Assembler::JMP(void* t) {
         if (!t) {
             JMP_long_placeholder();
         } else {
@@ -596,7 +627,6 @@ namespace nanojit
     }
 
     void Assembler::MR(Register rd, Register rs) {
-        underrunProtect(4);
         ORI(rs, 0, rd);
     }
 
@@ -613,7 +643,7 @@ namespace nanojit
         /**
          * Prologue
          */
-        underrunProtect(16);
+        SafeUnderrunProtect protect(this, 28);
         uint32_t stackNeeded = STACK_GRANULARITY * _activation.stackSlotsNeeded();
         uint32_t frameSize = stackNeeded + kcalleeAreaSize + kLinkageAreaSize;
         frameSize = BIT_ROUND_UP(frameSize, 8);
@@ -631,7 +661,7 @@ namespace nanojit
             outputf("        0x%x:",_nIns);
             outputf("        patch entry:");
         })
-        NIns *patchEntry = _nIns;
+        NIns* patchEntry = _nIns;
 
         // The frame size in SAVE is faked. We will still re-caculate SP later.
         // We can use 0 here but it is not good for debuggers.
@@ -643,7 +673,8 @@ namespace nanojit
         return patchEntry;
     }
 
-    void Assembler::asm_align_code() {
+    void Assembler::asm_align_code()
+    {
         while(uintptr_t(_nIns) & 15) {
             NOP();
         }
@@ -652,30 +683,27 @@ namespace nanojit
     void Assembler::nFragExit(LIns* guard)
     {
         SideExit* exit = guard->record()->exit;
-        Fragment *frag = exit->target;
-        GuardRecord *lr;
-        if (frag && frag->fragEntry)
-            {
-                JMP(frag->fragEntry);
-                lr = 0;
-            }
-        else
-            {
-                // Target doesn't exit yet. Emit jump to epilog, and set up to patch later.
-                if (!_epilogue)
-                    _epilogue = genEpilogue();
-                lr = guard->record();
-                JMP_long((intptr_t)_epilogue);
-                lr->jmp = _nIns;
-            }
+        Fragment* frag = exit->target;
+        GuardRecord* lr;
+        if (frag && frag->fragEntry) {
+            JMP(frag->fragEntry);
+            lr = 0;
+        } else {
+            // Target doesn't exit yet. Emit jump to epilog, and set up to patch later.
+            if (!_epilogue)
+                _epilogue = genEpilogue();
+            lr = guard->record();
+            JMP_long((intptr_t)_epilogue);
+            lr->jmp = _nIns;
+        }
 
         // return value is GuardRecord*
         SET32(int(lr), O0);
     }
 
-    NIns *Assembler::genEpilogue()
+    NIns* Assembler::genEpilogue()
     {
-        underrunProtect(12);
+        SafeUnderrunProtect protect(this, 12);
         RESTORE(G0, G0, G0); //restore
         JMPLI(I7, 8, G0); //ret
         ORI(O0, 0, I0);
@@ -685,19 +713,15 @@ namespace nanojit
     void Assembler::asm_call(LIns* ins)
     {
         if (!ins->isop(LIR_callv)) {
-            Register retReg = ( ins->isop(LIR_calld) ? F0 : retRegs[0] );
-            deprecated_prepResultReg(ins, rmask(retReg));
+            Register retReg = (ins->isop(LIR_calld) ? F0 : retRegs[0]);
+            prepareResultReg(ins, rmask(retReg));
+            evictScratchRegsExcept(rmask(retReg));
+        } else {
+            evictScratchRegsExcept(0);
         }
 
-        // Do this after we've handled the call result, so we don't
-        // force the call result to be spilled unnecessarily.
-        evictScratchRegsExcept(0);
-
         const CallInfo* ci = ins->callInfo();
 
-        underrunProtect(8);
-        NOP();
-
         ArgType argTypes[MAXARGS];
         uint32_t argc = ci->getArgTypes(argTypes);
 
@@ -705,53 +729,59 @@ namespace nanojit
                    ins->isop(LIR_calld));
         verbose_only(if (_logc->lcbits & LC_Native)
                      outputf("        0x%x:", _nIns);
-                     )
+                    )
         bool indirect = ci->isIndirect();
-        if (!indirect) {
-            CALL(ci);
-        }
-        else {
-            argc--;
-            Register r = findSpecificRegFor(ins->arg(argc), I0);
-            JMPL(G0, I0, O7);
+
+        {
+            SafeUnderrunProtect protect(this, 8);
+            NOP();
+            if (!indirect) {
+                CALL(ci);
+            }
+            else {
+                argc--;
+                Register r = findSpecificRegFor(ins->arg(argc), I0);
+                JMPL(G0, I0, O7);
+            }
         }
 
+        freeResourcesOf(ins);
+
         Register GPRIndex = O0;
         uint32_t offset = kLinkageAreaSize; // start of parameters stack postion.
 
-        for(int i=0; i<argc; i++)
-            {
-                uint32_t j = argc-i-1;
-                ArgType ty = argTypes[j];
-                if (ty == ARGTYPE_D) {
-                    Register r = findRegFor(ins->arg(j), FpRegs);
-
-                    underrunProtect(48);
-                    // We might be calling a varargs function.
-                    // So, make sure the GPR's are also loaded with
-                    // the value, or the stack contains it.
-                    if (GPRIndex <= O5) {
-                        LDSW32(SP, offset, GPRIndex);
-                    }
-                    GPRIndex = GPRIndex + 1;
-                    if (GPRIndex <= O5) {
-                        LDSW32(SP, offset+4, GPRIndex);
-                    }
-                    GPRIndex = GPRIndex + 1;
-                    STDF32(r, offset, SP);
-                    offset += 8;
+        for (int i = 0; i < argc; i++) {
+            uint32_t j = argc-i-1;
+            ArgType ty = argTypes[j];
+            if (ty == ARGTYPE_D) {
+                Register r = findRegFor(ins->arg(j), FpRegs);
+
+                SafeUnderrunProtect protect(this, 48);
+                // We might be calling a varargs function.
+                // So, make sure the GPR's are also loaded with
+                // the value, or the stack contains it.
+                if (GPRIndex <= O5) {
+                    LDSW32(SP, offset, GPRIndex);
+                }
+                GPRIndex = GPRIndex + 1;
+                if (GPRIndex <= O5) {
+                    LDSW32(SP, offset+4, GPRIndex);
+                }
+                GPRIndex = GPRIndex + 1;
+                STDF32(r, offset, SP);
+                offset += 8;
+            } else {
+                if (GPRIndex > O5) {
+                    SafeUnderrunProtect protect(this, 12);
+                    Register r = findRegFor(ins->arg(j), GpRegs);
+                    STW32(r, offset, SP);
                 } else {
-                    if (GPRIndex > O5) {
-                        underrunProtect(12);
-                        Register r = findRegFor(ins->arg(j), GpRegs);
-                        STW32(r, offset, SP);
-                    } else {
-                        Register r = findSpecificRegFor(ins->arg(j), GPRIndex);
-                    }
-                    GPRIndex = GPRIndex + 1;
-                    offset += 4;
+                    Register r = findSpecificRegFor(ins->arg(j), GPRIndex);
                 }
+                GPRIndex = GPRIndex + 1;
+                offset += 4;
             }
+        }
     }
 
     Register Assembler::nRegisterAllocFromSet(RegisterMask set)
@@ -772,10 +802,16 @@ namespace nanojit
 
     void Assembler::nPatchBranch(NIns* branch, NIns* location)
     {
-        *(uint32_t*)&branch[0] &= 0xFFC00000;
-        *(uint32_t*)&branch[0] |= ((intptr_t)location >> 10) & 0x3FFFFF;
-        *(uint32_t*)&branch[1] &= 0xFFFFFC00;
-        *(uint32_t*)&branch[1] |= (intptr_t)location & 0x3FF;
+        intptr_t addr_diff = ((intptr_t)location - (intptr_t)branch) >> 2;
+        if ( !isIMM22(addr_diff)) {
+            *(uint32_t*)&branch[0] = 0x05000000 | ((intptr_t)location >> 10) & 0x3FFFFF; // sethi ..., %g2
+            *(uint32_t*)&branch[1] = 0x8410a000 | (intptr_t)location & 0x3FF; // bset ..., %g2
+            *(uint32_t*)&branch[2] = 0x81c00002; // jmp %g2
+        } else {
+            *(uint32_t*)&branch[0] = 0x10800000 | (addr_diff & 0x3FFFFF); // ba
+            *(uint32_t*)&branch[1] = 0x01000000; // nop
+            *(uint32_t*)&branch[2] = 0x01000000; // nop
+        }
     }
 
     RegisterMask Assembler::nHint(LIns* ins)
@@ -792,10 +828,10 @@ namespace nanojit
 
     void Assembler::asm_restore(LIns* i, Register r)
     {
-        underrunProtect(24);
+        SafeUnderrunProtect protect(this, 24);
         if (i->isop(LIR_allocp)) {
             ADD(FP, L2, r);
-            int32_t d = deprecated_disp(i);
+            int32_t d = arDisp(i);
             SET32(d, L2);
         }
         else if (i->isImmI()) {
@@ -811,7 +847,7 @@ namespace nanojit
         }
     }
 
-    void Assembler::asm_store32(LOpcode op, LIns *value, int dr, LIns *base)
+    void Assembler::asm_store32(LOpcode op, LIns* value, int dr, LIns* base)
     {
         switch (op) {
             case LIR_sti:
@@ -824,53 +860,38 @@ namespace nanojit
                 return;
         }
 
-        underrunProtect(20);
-        if (value->isImmI())
-            {
-                Register rb = getBaseReg(base, dr, GpRegs);
-                int c = value->immI();
-                switch (op) {
-                case LIR_sti:
-                    STW32(L2, dr, rb);
-                    break;
-                case LIR_sti2c:
-                    STB32(L2, dr, rb);
-                    break;
-                case LIR_sti2s:
-                    STH32(L2, dr, rb);
-                    break;
-                }
-                SET32(c, L2);
+        SafeUnderrunProtect protect(this, 20);
+        if (value->isImmI()) {
+            Register rb = getBaseReg(base, dr, GpRegs);
+            int c = value->immI();
+            switch (op) {
+                case LIR_sti:   STW32(L2, dr, rb); break;
+                case LIR_sti2c: STB32(L2, dr, rb); break;
+                case LIR_sti2s: STH32(L2, dr, rb); break;
             }
-        else
-            {
-                // make sure what is in a register
-                Register ra, rb;
-                if (base->isImmI()) {
-                    // absolute address
-                    dr += base->immI();
-                    ra = findRegFor(value, GpRegs);
-                    rb = G0;
-                } else {
-                    getBaseReg2(GpRegs, value, ra, GpRegs, base, rb, dr);
-                }
-                switch (op) {
-                case LIR_sti:
-                    STW32(ra, dr, rb);
-                    break;
-                case LIR_sti2c:
-                    STB32(ra, dr, rb);
-                    break;
-                case LIR_sti2s:
-                    STH32(ra, dr, rb);
-                    break;
-                }
+            SET32(c, L2);
+        } else {
+            // make sure what is in a register
+            Register ra, rb;
+            if (base->isImmI()) {
+                // absolute address
+                dr += base->immI();
+                ra = findRegFor(value, GpRegs);
+                rb = G0;
+            } else {
+                getBaseReg2(GpRegs, value, ra, GpRegs, base, rb, dr);
+            }
+            switch (op) {
+                case LIR_sti:   STW32(ra, dr, rb); break;
+                case LIR_sti2c: STB32(ra, dr, rb); break;
+                case LIR_sti2s: STH32(ra, dr, rb); break;
             }
+        }
     }
 
     void Assembler::asm_spill(Register rr, int d, bool quad)
     {
-        underrunProtect(24);
+        SafeUnderrunProtect protect(this, 24);
         (void)quad;
         NanoAssert(d);
         if (rmask(rr) & FpRegs) {
@@ -892,7 +913,7 @@ namespace nanojit
                 return;
         }
 
-        underrunProtect(48);
+        SafeUnderrunProtect protect(this, 48);
         LIns* base = ins->oprnd1();
         int db = ins->disp();
         Register rb = getBaseReg(base, db, GpRegs);
@@ -937,7 +958,7 @@ namespace nanojit
                 return;
         }
 
-        underrunProtect(48);
+        SafeUnderrunProtect protect(this, 48);
         Register rb = getBaseReg(base, dr, GpRegs);
         if (op == LIR_std2f) {
             Register rv = ( !value->isInReg()
@@ -949,32 +970,30 @@ namespace nanojit
             return;
         }
 
-        if (value->isImmD())
-            {
-                // if a constant 64-bit value just store it now rather than
-                // generating a pointless store/load/store sequence
-                STW32(L2, dr+4, rb);
-                SET32(value->immDlo(), L2);
-                STW32(L2, dr, rb);
-                SET32(value->immDhi(), L2);
-                return;
-            }
+        if (value->isImmD()) {
+            // if a constant 64-bit value just store it now rather than
+            // generating a pointless store/load/store sequence
+            STW32(L2, dr+4, rb);
+            SET32(value->immDlo(), L2);
+            STW32(L2, dr, rb);
+            SET32(value->immDhi(), L2);
+            return;
+        }
 
-        if (value->isop(LIR_ldd))
-            {
-                // value is 64bit struct or int64_t, or maybe a double.
-                // it may be live in an FPU reg.  Either way, don't
-                // put it in an FPU reg just to load & store it.
+        if (value->isop(LIR_ldd)) {
+            // value is 64bit struct or int64_t, or maybe a double.
+            // it may be live in an FPU reg.  Either way, don't
+            // put it in an FPU reg just to load & store it.
 
-                // a) if we know it's not a double, this is right.
-                // b) if we guarded that its a double, this store could be on
-                // the side exit, copying a non-double.
-                // c) maybe its a double just being stored.  oh well.
+            // a) if we know it's not a double, this is right.
+            // b) if we guarded that its a double, this store could be on
+            // the side exit, copying a non-double.
+            // c) maybe its a double just being stored.  oh well.
 
-                int da = findMemFor(value);
-                asm_mmq(rb, dr, FP, da);
-                return;
-            }
+            int da = findMemFor(value);
+             asm_mmq(rb, dr, FP, da);
+             return;
+        }
 
         // if value already in a reg, use that, otherwise
         // get it into FPU regs.
@@ -1005,66 +1024,56 @@ namespace nanojit
         NIns* at = 0;
         LOpcode condop = cond->opcode();
         NanoAssert(cond->isCmp());
-        if (isCmpDOpcode(condop))
-            {
-                return asm_branchd(branchOnFalse, cond, targ);
-            }
-
-        underrunProtect(32);
-        intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
-        // !targ means that it needs patch.
-        if( !(isIMM22((int32_t)tt)) || !targ ) {
-            JMP_long_nocheck((intptr_t)targ);
-            at = _nIns;
-            NOP();
-            BA(0, 5);
-            tt = 4;
+        if (isCmpDOpcode(condop)) {
+            return asm_branchd(branchOnFalse, cond, targ);
         }
-        NOP();
 
-        // produce the branch
-        if (branchOnFalse)
-            {
-                if (condop == LIR_eqi)
-                    BNE(0, tt);
-                else if (condop == LIR_lti)
-                    BGE(0, tt);
-                else if (condop == LIR_lei)
-                    BG(0, tt);
-                else if (condop == LIR_gti)
-                    BLE(0, tt);
-                else if (condop == LIR_gei)
-                    BL(0, tt);
-                else if (condop == LIR_ltui)
-                    BCC(0, tt);
-                else if (condop == LIR_leui)
-                    BGU(0, tt);
-                else if (condop == LIR_gtui)
-                    BLEU(0, tt);
-                else //if (condop == LIR_geui)
-                    BCS(0, tt);
+        {
+            SafeUnderrunProtect protect(this, 32);
+            intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
+            // !targ means that it needs patch.
+            if( !(isIMM22((int32_t)tt)) || !targ ) {
+                JMP_long((intptr_t)targ);
+                at = _nIns;
+                NOP();
+                BA(0, 5);
+                tt = 4;
             }
-        else // op == LIR_xt
-            {
-                if (condop == LIR_eqi)
-                    BE(0, tt);
-                else if (condop == LIR_lti)
-                    BL(0, tt);
-                else if (condop == LIR_lei)
-                    BLE(0, tt);
-                else if (condop == LIR_gti)
-                    BG(0, tt);
-                else if (condop == LIR_gei)
-                    BGE(0, tt);
-                else if (condop == LIR_ltui)
-                    BCS(0, tt);
-                else if (condop == LIR_leui)
-                    BLEU(0, tt);
-                else if (condop == LIR_gtui)
-                    BGU(0, tt);
-                else //if (condop == LIR_geui)
-                    BCC(0, tt);
+            NOP();
+
+            // produce the branch
+            if (branchOnFalse) {
+                switch (condop) {
+                    case LIR_eqi : BNE (0, tt); break;
+                    case LIR_lti : BGE (0, tt); break;
+                    case LIR_lei : BG  (0, tt); break;
+                    case LIR_gti : BLE (0, tt); break;
+                    case LIR_gei : BL  (0, tt); break;
+                    case LIR_ltui: BCC (0, tt); break;
+                    case LIR_leui: BGU (0, tt); break;
+                    case LIR_gtui: BLEU(0, tt); break;
+                    case LIR_geui: BCS (0, tt); break;
+                    default:
+                        NanoAssertMsg(0, "asm_branch should never receive this cond opcode");
+                        return;
+                }
+            } else {// op == LIR_xt
+                switch (condop) {
+                    case LIR_eqi : BE  (0, tt); break;
+                    case LIR_lti : BL  (0, tt); break;
+                    case LIR_lei : BLE (0, tt); break;
+                    case LIR_gti : BG  (0, tt); break;
+                    case LIR_gei : BGE (0, tt); break;
+                    case LIR_ltui: BCS (0, tt); break;
+                    case LIR_leui: BLEU(0, tt); break;
+                    case LIR_gtui: BGU (0, tt); break;
+                    case LIR_geui: BCC (0, tt); break;
+                    default:
+                        NanoAssertMsg(0, "asm_branch should never receive this cond opcode");
+                        return;
+                }
             }
+        }
         asm_cmp(cond);
         return at;
     }
@@ -1072,11 +1081,11 @@ namespace nanojit
     NIns* Assembler::asm_branch_ov(LOpcode op, NIns* targ)
     {
         NIns* at = 0;
-        underrunProtect(32);
+        SafeUnderrunProtect protect(this, 32);
         intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
         // !targ means that it needs patch.
-        if( !(isIMM22((int32_t)tt)) || !targ ) {
-            JMP_long_nocheck((intptr_t)targ);
+        if (!(isIMM22((int32_t)tt)) || !targ) {
+            JMP_long((intptr_t)targ);
             at = _nIns;
             NOP();
             BA(0, 5);
@@ -1084,242 +1093,264 @@ namespace nanojit
         }
         NOP();
 
-        if( op == LIR_mulxovi || op == LIR_muljovi )
+        if (op == LIR_mulxovi || op == LIR_muljovi)
             BNE(0, tt);
         else
             BVS(0, tt);
         return at;
     }
 
-    void Assembler::asm_cmp(LIns *cond)
+    void Assembler::asm_cmp(LIns* cond)
     {
-        underrunProtect(12);
-
         LIns* lhs = cond->oprnd1();
         LIns* rhs = cond->oprnd2();
 
         NanoAssert(lhs->isI() && rhs->isI());
 
         // ready to issue the compare
-        if (rhs->isImmI())
-            {
-                int c = rhs->immI();
-                Register r = findRegFor(lhs, GpRegs);
-                if (c == 0 && cond->isop(LIR_eqi)) {
-                    ANDCC(r, r, G0);
-                }
-                else {
-                    SUBCC(r, L2, G0);
-                    SET32(c, L2);
-                }
-            }
-        else
-            {
-                Register ra, rb;
-                findRegFor2(GpRegs, lhs, ra, GpRegs, rhs, rb);
-                SUBCC(ra, rb, G0);
+        if (rhs->isImmI()) {
+            int c = rhs->immI();
+            Register r = findRegFor(lhs, GpRegs);
+            SafeUnderrunProtect protect(this, 12);
+            if (c == 0 && cond->isop(LIR_eqi)) {
+                ANDCC(r, r, G0);
+            } else {
+                SUBCC(r, L2, G0);
+                SET32(c, L2);
             }
+        } else {
+            Register ra, rb;
+            findRegFor2(GpRegs, lhs, ra, GpRegs, rhs, rb);
+            SUBCC(ra, rb, G0);
+        }
     }
 
     void Assembler::asm_condd(LIns* ins)
     {
         // only want certain regs
-        Register r = deprecated_prepResultReg(ins, AllowableFlagRegs);
-        underrunProtect(8);
+        Register r = prepareResultReg(ins, AllowableFlagRegs);
+        SafeUnderrunProtect protect(this, 12);
         LOpcode condop = ins->opcode();
         NanoAssert(isCmpDOpcode(condop));
-        if (condop == LIR_eqd)
-            MOVFEI(1, r);
-        else if (condop == LIR_led)
-            MOVFLEI(1, r);
-        else if (condop == LIR_ltd)
-            MOVFLI(1, r);
-        else if (condop == LIR_ged)
-            MOVFGEI(1, r);
-        else // if (condop == LIR_gtd)
-            MOVFGI(1, r);
+        switch (condop) {
+            case LIR_eqd: MOVFEI (1, r); break;
+            case LIR_led: MOVFLEI(1, r); break;
+            case LIR_ltd: MOVFLI (1, r); break;
+            case LIR_ged: MOVFGEI(1, r); break;
+            case LIR_gtd: MOVFGI (1, r); break;
+            default:
+                NanoAssertMsg(0, "asm_condd should never receive this cond opcode");
+                return;
+        }
         ORI(G0, 0, r);
+        freeResourcesOf(ins);
         asm_cmpd(ins);
     }
 
     void Assembler::asm_cond(LIns* ins)
     {
-        underrunProtect(8);
         // only want certain regs
-        LOpcode op = ins->opcode();
-        Register r = deprecated_prepResultReg(ins, AllowableFlagRegs);
-
-        if (op == LIR_eqi)
-            MOVEI(1, r);
-        else if (op == LIR_lti)
-            MOVLI(1, r);
-        else if (op == LIR_lei)
-            MOVLEI(1, r);
-        else if (op == LIR_gti)
-            MOVGI(1, r);
-        else if (op == LIR_gei)
-            MOVGEI(1, r);
-        else if (op == LIR_ltui)
-            MOVCSI(1, r);
-        else if (op == LIR_leui)
-            MOVLEUI(1, r);
-        else if (op == LIR_gtui)
-            MOVGUI(1, r);
-        else // if (op == LIR_geui)
-            MOVCCI(1, r);
+        LOpcode condop = ins->opcode();
+        Register r = prepareResultReg(ins, AllowableFlagRegs);
+        SafeUnderrunProtect protect(this, 20);
+
+        switch (condop) {
+            case LIR_eqi : MOVEI  (1, r); break;
+            case LIR_lti : MOVLI  (1, r); break;
+            case LIR_lei : MOVLEI (1, r); break;
+            case LIR_gti : MOVGI  (1, r); break;
+            case LIR_gei : MOVGEI (1, r); break;
+            case LIR_ltui: MOVCSI (1, r); break;
+            case LIR_leui: MOVLEUI(1, r); break;
+            case LIR_gtui: MOVGUI (1, r); break;
+            case LIR_geui: MOVCCI (1, r); break;
+            default:
+                NanoAssertMsg(0, "asm_cond should never receive this cond opcode");
+                return;
+        }
         ORI(G0, 0, r);
+        freeResourcesOf(ins);
         asm_cmp(ins);
     }
 
     void Assembler::asm_arith(LIns* ins)
     {
-        underrunProtect(28);
+        SafeUnderrunProtect protect(this, 28);
         LOpcode op = ins->opcode();
         LIns* lhs = ins->oprnd1();
         LIns* rhs = ins->oprnd2();
 
-        Register rb = deprecated_UnknownReg;
+        Register rb;
         RegisterMask allow = GpRegs;
         bool forceReg = (op == LIR_muli || op == LIR_mulxovi || op == LIR_muljovi || !rhs->isImmI());
 
-        if (lhs != rhs && forceReg)
-            {
-                if ((rb = asm_binop_rhs_reg(ins)) == deprecated_UnknownReg) {
-                    rb = findRegFor(rhs, allow);
-                }
-                allow &= ~rmask(rb);
-            }
-        else if ((op == LIR_addi || op == LIR_addxovi || op == LIR_addjovi) && lhs->isop(LIR_allocp) && rhs->isImmI()) {
+        if (lhs != rhs && forceReg) {
+            rb = findRegFor(rhs, allow);
+            allow &= ~rmask(rb);
+        } else if ((op == LIR_addi || op == LIR_addxovi || op == LIR_addjovi) && lhs->isop(LIR_allocp) && rhs->isImmI()) {
             // add alloc+const, use lea
-            Register rr = deprecated_prepResultReg(ins, allow);
+            Register rr = prepareResultReg(ins, allow);
             int d = findMemFor(lhs) + rhs->immI();
+            SafeUnderrunProtect protect(this, 12);
             ADD(FP, L2, rr);
             SET32(d, L2);
+            freeResourcesOf(ins);
             return;
         }
 
-        Register rr = deprecated_prepResultReg(ins, allow);
-        // if this is last use of lhs in reg, we can re-use result reg
-        // else, lhs already has a register assigned.
-        Register ra = ( !lhs->isInReg()
-                      ? findSpecificRegFor(lhs, rr)
-                      : lhs->deprecated_getReg() );
+        Register rr = prepareResultReg(ins, allow);
 
-        if (forceReg)
-            {
-                if (lhs == rhs)
-                    rb = ra;
+        // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
+        Register ra = lhs->isInReg() ? lhs->getReg() : rr;
+
+        if (forceReg) {
+            if (lhs == rhs)
+                rb = ra;
 
-                if (op == LIR_addi || op == LIR_addxovi || op == LIR_addjovi)
+            switch (op) {
+                case LIR_addi:
+                case LIR_addxovi:
+                case LIR_addjovi:
                     ADDCC(rr, rb, rr);
-                else if (op == LIR_subi || op == LIR_subxovi || op == LIR_subjovi)
+                    break;
+                case LIR_subi:
+                case LIR_subxovi:
+                case LIR_subjovi:
                     SUBCC(rr, rb, rr);
-                else if (op == LIR_muli)
+                    break;
+                case LIR_muli:
                     SMULCC(rr, rb, rr);
-                else if (op == LIR_mulxovi || op == LIR_muljovi) {
+                    break;
+                case LIR_mulxovi:
+                case LIR_muljovi:
                     SUBCC(L4, L6, L4);
                     SRAI(rr, 31, L6);
                     RDY(L4);
                     SMULCC(rr, rb, rr);
-                }
-                else if (op == LIR_andi)
+                    break;
+                case LIR_andi:
                     AND(rr, rb, rr);
-                else if (op == LIR_ori)
+                    break;
+                case LIR_ori:
                     OR(rr, rb, rr);
-                else if (op == LIR_xori)
+                    break;
+                case LIR_xori:
                     XOR(rr, rb, rr);
-                else if (op == LIR_lshi)
+                    break;
+                case LIR_lshi:
                     SLL(rr, rb, rr);
-                else if (op == LIR_rshi)
+                    break;
+                case LIR_rshi:
                     SRA(rr, rb, rr);
-                else if (op == LIR_rshui)
+                    break;
+                case LIR_rshui:
                     SRL(rr, rb, rr);
-                else
+                    break;
+                default:
                     NanoAssertMsg(0, "Unsupported");
+                    return;
             }
-        else
-            {
-                int c = rhs->immI();
-                if (op == LIR_addi || op == LIR_addxovi || op == LIR_addjovi)
+        } else {
+            int c = rhs->immI();
+            switch (op) {
+                case LIR_addi:
+                case LIR_addxovi:
+                case LIR_addjovi:
                     ADDCC(rr, L2, rr);
-                else if (op == LIR_subi || op == LIR_subxovi || op == LIR_subjovi)
+                    break;
+                case LIR_subi:
+                case LIR_subxovi:
+                case LIR_subjovi:
                     SUBCC(rr, L2, rr);
-                else if (op == LIR_andi)
+                    break;
+                case LIR_andi:
                     AND(rr, L2, rr);
-                else if (op == LIR_ori)
+                    break;
+                case LIR_ori:
                     OR(rr, L2, rr);
-                else if (op == LIR_xori)
+                    break;
+                case LIR_xori:
                     XOR(rr, L2, rr);
-                else if (op == LIR_lshi)
+                    break;
+                case LIR_lshi:
                     SLL(rr, L2, rr);
-                else if (op == LIR_rshi)
+                    break;
+                case LIR_rshi:
                     SRA(rr, L2, rr);
-                else if (op == LIR_rshui)
+                    break;
+                case LIR_rshui:
                     SRL(rr, L2, rr);
-                else
+                    break;
+                default:
                     NanoAssertMsg(0, "Unsupported");
-                SET32(c, L2);
+                    return;
             }
+            SET32(c, L2);
+        }
 
-        if ( rr != ra )
+        if (rr != ra)
             ORI(ra, 0, rr);
+
+        freeResourcesOf(ins);
+        if (!lhs->isInReg()) {
+            NanoAssert(ra == rr);
+            findSpecificRegForUnallocated(lhs, ra);
+        }
     }
 
     void Assembler::asm_neg_not(LIns* ins)
     {
-        underrunProtect(8);
         LOpcode op = ins->opcode();
-        Register rr = deprecated_prepResultReg(ins, GpRegs);
+        Register rr = prepareResultReg(ins, GpRegs);
 
+        SafeUnderrunProtect protect(this, 16);
         LIns* lhs = ins->oprnd1();
-        // if this is last use of lhs in reg, we can re-use result reg
-        // else, lhs already has a register assigned.
-        Register ra = ( !lhs->isInReg()
-                      ? findSpecificRegFor(lhs, rr)
-                      : lhs->deprecated_getReg() );
+        // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
+        Register ra = lhs->isInReg() ? lhs->getReg() : rr;
 
         if (op == LIR_noti)
             ORN(G0, rr, rr);
         else
             SUB(G0, rr, rr);
 
-        if ( rr != ra )
+        if (rr != ra)
             ORI(ra, 0, rr);
+
+        freeResourcesOf(ins);
+        if (!lhs->isInReg()) {
+            NanoAssert(ra == rr);
+            findSpecificRegForUnallocated(lhs, ra);
+        }
     }
 
     void Assembler::asm_load32(LIns* ins)
     {
-        underrunProtect(12);
         LOpcode op = ins->opcode();
         LIns* base = ins->oprnd1();
         int d = ins->disp();
-        Register rr = deprecated_prepResultReg(ins, GpRegs);
+        Register rr = prepareResultReg(ins, GpRegs);
         Register ra = getBaseReg(base, d, GpRegs);
+
+        SafeUnderrunProtect protect(this, 12);
         switch(op) {
-            case LIR_lduc2ui:
-                LDUB32(ra, d, rr);
-                break;
-            case LIR_ldus2ui:
-                LDUH32(ra, d, rr);
-                break;
-            case LIR_ldi:
-                LDSW32(ra, d, rr);
-                break;
-            case LIR_ldc2i:
-                LDSB32(ra, d, rr);
-                break;
-            case LIR_lds2i:
-                LDSH32(ra, d, rr);
-                break;
+            case LIR_lduc2ui: LDUB32(ra, d, rr); break;
+            case LIR_ldus2ui: LDUH32(ra, d, rr); break;
+            case LIR_ldi    : LDSW32(ra, d, rr); break;
+            case LIR_ldc2i  : LDSB32(ra, d, rr); break;
+            case LIR_lds2i  : LDSH32(ra, d, rr); break;
             default:
                 NanoAssertMsg(0, "asm_load32 should never receive this LIR opcode");
                 return;
         }
+        freeResourcesOf(ins);
+        if (!base->isop(LIR_allocp) && !base->isInReg()) {
+            NanoAssert(ra == rr);
+            findSpecificRegForUnallocated(base, ra);
+        }
     }
 
     void Assembler::asm_cmov(LIns* ins)
     {
-        underrunProtect(4);
         LOpcode op = ins->opcode();
         LIns* condval = ins->oprnd1();
         LIns* iftrue  = ins->oprnd2();
@@ -1330,7 +1361,7 @@ namespace nanojit
                   (op == LIR_cmovd && iftrue->isD() && iffalse->isD()));
 
         RegisterMask rm = (op == LIR_cmovi) ? GpRegs : FpRegs;
-        const Register rr = deprecated_prepResultReg(ins, rm);
+        const Register rr = prepareResultReg(ins, rm);
         const Register iffalsereg = findRegFor(iffalse, rm & ~rmask(rr));
         bool isIcc = true;
 
@@ -1346,7 +1377,9 @@ namespace nanojit
                 case LIR_leui: MOVGU (iffalsereg, rr); break;
                 case LIR_gtui: MOVLEU(iffalsereg, rr); break;
                 case LIR_geui: MOVCS (iffalsereg, rr); break;
-                debug_only( default: NanoAssert(0); break; )
+                default:
+                    NanoAssertMsg(0, "asm_comv should never receive this cond opcode");
+                    return;
             }
         } else {
             switch (condval->opcode()) {
@@ -1365,10 +1398,13 @@ namespace nanojit
                 case LIR_ltd:  FMOVDFUGE(iffalsereg, rr); isIcc = false; break;
                 case LIR_ged:  FMOVDFUL (iffalsereg, rr); isIcc = false; break;
                 case LIR_gtd:  FMOVDFULE(iffalsereg, rr); isIcc = false; break;
-                debug_only( default: NanoAssert(0); break; )
+                default:
+                    NanoAssertMsg(0, "asm_comv should never receive this cond opcode");
+                    return;
             }
         }
 
+        freeResourcesOf(ins);
         /*const Register iftruereg =*/ findSpecificRegFor(iftrue, rr);
         if (isIcc)
             asm_cmp(condval);
@@ -1379,16 +1415,16 @@ namespace nanojit
 
     void Assembler::asm_param(LIns* ins)
     {
-        underrunProtect(12);
         uint32_t a = ins->paramArg();
         NanoAssertMsg(ins->paramKind() == 0, "savedRegs are not used on SPARC");
 
-        if (a < sizeof(argRegs)/sizeof(argRegs[0])) { // i0 - i5
+        if (a < sizeof(argRegs) / sizeof(argRegs[0])) { // i0 - i5
             prepareResultReg(ins, rmask(argRegs[a]));
         } else {
             // Incoming arg is on stack
             Register r = prepareResultReg(ins, GpRegs);
             int32_t d = a * sizeof (intptr_t) + kLinkageAreaSize;
+            SafeUnderrunProtect protect(this, 12);
             LDSW32(FP, d, r);
         }
         freeResourcesOf(ins);
@@ -1396,100 +1432,102 @@ namespace nanojit
 
     void Assembler::asm_immi(LIns* ins)
     {
-        underrunProtect(8);
-        Register rr = deprecated_prepResultReg(ins, GpRegs);
+        Register rr = prepareResultReg(ins, GpRegs);
+        SafeUnderrunProtect protect(this, 12);
         int32_t val = ins->immI();
         if (val == 0)
             XOR(rr, rr, rr);
         else
             SET32(val, rr);
+        freeResourcesOf(ins);
     }
 
     void Assembler::asm_immd(LIns* ins)
     {
-        underrunProtect(64);
-        Register rr = ins->deprecated_getReg();
-        if (rr != deprecated_UnknownReg)
-            {
-                // @todo -- add special-cases for 0 and 1
-                _allocator.retire(rr);
-                ins->clearReg();
-                NanoAssert((rmask(rr) & FpRegs) != 0);
-                findMemFor(ins);
-                int d = deprecated_disp(ins);
-                LDDF32(FP, d, rr);
-            }
+        SafeUnderrunProtect protect(this, 64);
+        if (ins->isInReg()) {
+            Register rr = ins->getReg();
+            // @todo -- add special-cases for 0 and 1
+            _allocator.retire(rr);
+            ins->clearReg();
+            NanoAssert((rmask(rr) & FpRegs) != 0);
+            findMemFor(ins);
+            int d = arDisp(ins);
+            LDDF32(FP, d, rr);
+        }
 
         // @todo, if we used xor, ldsd, fldz, etc above, we don't need mem here
-        int d = deprecated_disp(ins);
-        deprecated_freeRsrcOf(ins);
-        if (d)
-            {
-                STW32(L2, d+4, FP);
-                SET32(ins->immDlo(), L2);
-                STW32(L2, d, FP);
-                SET32(ins->immDhi(), L2);
-            }
+        int d = arDisp(ins);
+        if (d) {
+            STW32(L2, d+4, FP);
+            SET32(ins->immDlo(), L2);
+            STW32(L2, d, FP);
+            SET32(ins->immDhi(), L2);
+        }
+        freeResourcesOf(ins);
     }
 
     void Assembler::asm_fneg(LIns* ins)
     {
-        underrunProtect(4);
-        Register rr = deprecated_prepResultReg(ins, FpRegs);
+        Register rr = prepareResultReg(ins, FpRegs);
         LIns* lhs = ins->oprnd1();
 
-        // lhs into reg, prefer same reg as result
-        // if this is last use of lhs in reg, we can re-use result reg
-        // else, lhs already has a different reg assigned
-        Register ra = ( !lhs->isInReg()
-                      ? findSpecificRegFor(lhs, rr)
-                      : findRegFor(lhs, FpRegs) );
+        // If 'lhs' isn't in a register, it can be clobbered by 'ins'.
+        Register ra = lhs->isInReg() ? lhs->getReg() : rr;
 
         FNEGD(ra, rr);
+
+        freeResourcesOf(ins);
+        if (!lhs->isInReg()) {
+            NanoAssert(ra == rr);
+            findSpecificRegForUnallocated(lhs, ra);
+        }
     }
 
     void Assembler::asm_fop(LIns* ins)
     {
-        underrunProtect(4);
         LOpcode op = ins->opcode();
-        LIns *lhs = ins->oprnd1();
-        LIns *rhs = ins->oprnd2();
+        LIns* lhs = ins->oprnd1();
+        LIns* rhs = ins->oprnd2();
 
         RegisterMask allow = FpRegs;
         Register ra, rb;
         findRegFor2(allow, lhs, ra, allow, rhs, rb);
-        Register rr = deprecated_prepResultReg(ins, allow);
+        Register rr = prepareResultReg(ins, allow);
 
-        if (op == LIR_addd)
-            FADDD(ra, rb, rr);
-        else if (op == LIR_subd)
-            FSUBD(ra, rb, rr);
-        else if (op == LIR_muld)
-            FMULD(ra, rb, rr);
-        else //if (op == LIR_divd)
-            FDIVD(ra, rb, rr);
+        switch (op) {
+            case LIR_addd: FADDD(ra, rb, rr); break;
+            case LIR_subd: FSUBD(ra, rb, rr); break;
+            case LIR_muld: FMULD(ra, rb, rr); break;
+            case LIR_divd: FDIVD(ra, rb, rr); break;
+            default:
+                NanoAssertMsg(0, "asm_fop should never receive this opcode");
+                return;
+        }
 
+        freeResourcesOf(ins);
     }
 
     void Assembler::asm_i2d(LIns* ins)
     {
-        underrunProtect(32);
         // where our result goes
-        Register rr = deprecated_prepResultReg(ins, FpRegs);
+        Register rr = prepareResultReg(ins, FpRegs);
+        SafeUnderrunProtect protect(this, 32);
         int d = findMemFor(ins->oprnd1());
         FITOD(rr, rr);
         LDDF32(FP, d, rr);
+        freeResourcesOf(ins);
     }
 
     void Assembler::asm_ui2d(LIns* ins)
     {
-        underrunProtect(72);
         // where our result goes
-        Register rr = deprecated_prepResultReg(ins, FpRegs);
+        Register rr = prepareResultReg(ins, FpRegs);
         Register rt = registerAllocTmp(FpRegs & ~(rmask(rr)));
         Register gr = findRegFor(ins->oprnd1(), GpRegs);
         int disp = -8;
 
+        SafeUnderrunProtect protect(this, 72);
         FABSS(rr, rr);
         FSUBD(rt, rr, rr);
         LDDF32(SP, disp, rr);
@@ -1498,37 +1536,37 @@ namespace nanojit
         STWI(gr, disp+4, SP);
         STWI(G1, disp, SP);
         SETHI(0x43300000, G1);
+        freeResourcesOf(ins);
     }
 
     void Assembler::asm_d2i(LIns* ins) {
-        underrunProtect(28);
-        LIns *lhs = ins->oprnd1();
+        LIns* lhs = ins->oprnd1();
         Register rr = prepareResultReg(ins, GpRegs);
         Register ra = findRegFor(lhs, FpRegs);
         int d = findMemFor(ins);
+        SafeUnderrunProtect protect(this, 28);
         LDSW32(FP, d, rr);
-        STF32(ra, d, FP);
-        FDTOI(ra, ra);
+        STF32(F28, d, FP);
+        FDTOI(ra, F28);
         freeResourcesOf(ins);
     }
 
     void Assembler::asm_nongp_copy(Register r, Register s)
     {
-        underrunProtect(4);
         NanoAssert((rmask(r) & FpRegs) && (rmask(s) & FpRegs));
         FMOVD(s, r);
     }
 
-    NIns * Assembler::asm_branchd(bool branchOnFalse, LIns *cond, NIns *targ)
+    NIns* Assembler::asm_branchd(bool branchOnFalse, LIns* cond, NIns* targ)
     {
-        NIns *at = 0;
+        NIns* at = 0;
         LOpcode condop = cond->opcode();
         NanoAssert(isCmpDOpcode(condop));
-        underrunProtect(32);
+        SafeUnderrunProtect protect(this, 32);
         intptr_t tt = ((intptr_t)targ - (intptr_t)_nIns + 8) >> 2;
         // !targ means that it needs patch.
         if( !(isIMM22((int32_t)tt)) || !targ ) {
-            JMP_long_nocheck((intptr_t)targ);
+            JMP_long((intptr_t)targ);
             at = _nIns;
             NOP();
             BA(0, 5);
@@ -1537,39 +1575,35 @@ namespace nanojit
         NOP();
 
         // produce the branch
-        if (branchOnFalse)
-            {
-                if (condop == LIR_eqd)
-                    FBNE(0, tt);
-                else if (condop == LIR_led)
-                    FBUG(0, tt);
-                else if (condop == LIR_ltd)
-                    FBUGE(0, tt);
-                else if (condop == LIR_ged)
-                    FBUL(0, tt);
-                else //if (condop == LIR_gtd)
-                    FBULE(0, tt);
+        if (branchOnFalse) {
+            switch (condop) {
+                case LIR_eqd: FBNE (0, tt); break;
+                case LIR_led: FBUG (0, tt); break;
+                case LIR_ltd: FBUGE(0, tt); break;
+                case LIR_ged: FBUL (0, tt); break;
+                case LIR_gtd: FBULE(0, tt); break;
+                default:
+                    NanoAssertMsg(0, "asm_branchd should never receive this cond opcode");
+                    return;
             }
-        else // op == LIR_xt
-            {
-                if (condop == LIR_eqd)
-                    FBE(0, tt);
-                else if (condop == LIR_led)
-                    FBLE(0, tt);
-                else if (condop == LIR_ltd)
-                    FBL(0, tt);
-                else if (condop == LIR_ged)
-                    FBGE(0, tt);
-                else //if (condop == LIR_gtd)
-                    FBG(0, tt);
+        } else { // op == LIR_xt
+            switch (condop) {
+                case LIR_eqd: FBE (0, tt); break;
+                case LIR_led: FBLE(0, tt); break;
+                case LIR_ltd: FBL (0, tt); break;
+                case LIR_ged: FBGE(0, tt); break;
+                case LIR_gtd: FBG (0, tt); break;
+                default:
+                    NanoAssertMsg(0, "asm_branchd should never receive this cond opcode");
+                    return;
             }
+        }
         asm_cmpd(cond);
         return at;
     }
 
-    void Assembler::asm_cmpd(LIns *cond)
+    void Assembler::asm_cmpd(LIns* cond)
     {
-        underrunProtect(4);
         LIns* lhs = cond->oprnd1();
         LIns* rhs = cond->oprnd2();
 
@@ -1583,11 +1617,6 @@ namespace nanojit
     {
     }
 
-    Register Assembler::asm_binop_rhs_reg(LIns* ins)
-    {
-        return deprecated_UnknownReg;
-    }
-
     void Assembler::nativePageSetup()
     {
         NanoAssert(!_inExit);
@@ -1607,11 +1636,12 @@ namespace nanojit
     void
     Assembler::underrunProtect(int n)
     {
-        NIns *eip = _nIns;
+        NIns* eip = _nIns;
+        NanoAssertMsg(n <= LARGEST_UNDERRUN_PROT, "constant LARGEST_UNDERRUN_PROT is too small %d");
         // This may be in a normal code chunk or an exit code chunk.
-        if (eip - n < codeStart) {
+        if ((intptr_t)eip - n < (intptr_t)codeStart) {
             codeAlloc(codeStart, codeEnd, _nIns verbose_only(, codeBytes));
-            JMP_long_nocheck((intptr_t)eip);
+            JMP_long((intptr_t)eip);
         }
     }
 
@@ -1620,7 +1650,7 @@ namespace nanojit
         genEpilogue();
         releaseRegisters();
         assignSavedRegs();
-        LIns *val = ins->oprnd1();
+        LIns* val = ins->oprnd1();
         if (ins->isop(LIR_reti)) {
             findSpecificRegFor(val, retRegs[0]);
         } else {
diff --git a/js/src/nanojit/NativeSparc.h b/js/src/nanojit/NativeSparc.h
index 2a69caa..462ffc8 100644
--- a/nanojit/NativeSparc.h
+++ b/nanojit/NativeSparc.h
@@ -71,7 +71,7 @@ namespace nanojit
 {
     const int NJ_MAX_REGISTERS = 30; // L0 - L7, I0 - I5, F2 - F14
 
-    const int LARGEST_UNDERRUN_PROT = 32;  // largest value passed to underrunProtect
+    const int LARGEST_UNDERRUN_PROT = 72;  // largest value passed to underrunProtect
 
 #define NJ_MAX_STACK_ENTRY              8192
 #define NJ_MAX_PARAMETERS               1
@@ -203,6 +203,7 @@ namespace nanojit
 #define DECLARE_PLATFORM_REGALLOC()
 
 #define DECLARE_PLATFORM_ASSEMBLER()    \
+     friend class SafeUnderrunProtect; \
      const static Register argRegs[6], retRegs[1]; \
      bool has_cmov; \
      void nativePageReset(); \
@@ -210,10 +211,11 @@ namespace nanojit
      void underrunProtect(int bytes); \
      bool hardenNopInsertion(const Config& /*c*/) { return false; } \
      void asm_align_code(); \
-     void asm_cmp(LIns *cond); \
-     void asm_cmpd(LIns *cond); \
+     void asm_cmp(LIns* cond); \
+     void asm_cmpd(LIns* cond); \
      NIns* asm_branchd(bool, LIns*, NIns*); \
      void IMM32(int32_t i) { \
+         underrunProtect(4); \
          --_nIns; \
          *((int32_t*)_nIns) = i; \
      } \
@@ -295,16 +297,16 @@ namespace nanojit
     void Format_4_5(Register rd, int32_t op3, int32_t cond, int32_t opf_cc, int32_t opf_low, Register rs2) { \
         Format_3A(2, rd, op3, (cond & 0xF) << 14 | (opf_cc & 0x7) << 11 | (opf_low & 0x3F) << 5 | _reg_(rs2)); \
     } \
-    void IntegerOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char *opcode); \
-    void IntegerOperationI(Register rs1, int32_t simm13, Register rd, int32_t op3, const char *opcode); \
-    void FloatOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char *opcode); \
-    void Bicc(int32_t a, int32_t dsp22, int32_t cond, const char *opcode); \
-    void FBfcc(int32_t a, int32_t dsp22, int32_t cond, const char *opcode); \
+    void IntegerOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char* opcode); \
+    void IntegerOperationI(Register rs1, int32_t simm13, Register rd, int32_t op3, const char* opcode); \
+    void FloatOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char* opcode); \
+    void Bicc(int32_t a, int32_t dsp22, int32_t cond, const char* opcode); \
+    void FBfcc(int32_t a, int32_t dsp22, int32_t cond, const char* opcode); \
     void LoadOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char* opcode); \
     void LoadOperationI(Register rs1, int32_t simm13, Register rd, int32_t op3, const char* opcode); \
-    void MOVcc(Register rs, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char *opcode); \
-    void MOVccI(int32_t simm11, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char *opcode); \
-    void FMOVDcc(Register rs, int32_t opt_cc, Register rd, int32_t cond, const char *opcode); \
+    void MOVcc(Register rs, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char* opcode); \
+    void MOVccI(int32_t simm11, int32_t cc2, int32_t cc1, int32_t cc0, Register rd, int32_t cond, const char* opcode); \
+    void FMOVDcc(Register rs, int32_t opt_cc, Register rd, int32_t cond, const char* opcode); \
     void ShiftOperation(Register rs1, Register rs2, Register rd, int32_t op3, const char* opcode); \
     void ShiftOperationI(Register rs1, int32_t shcnt32, Register rd, int32_t op3, const char* opcode); \
     void Store(Register rd, Register rs1, Register rs2, int32_t op3, const char* opcode); \
@@ -443,14 +445,13 @@ namespace nanojit
     void STB(Register rd, Register rs1, Register rs2); \
     void STBI(Register rd, int32_t simm13, Register rs1); \
     void STB32(Register rd, int32_t immI, Register rs1); \
-    bool isIMM13(int32_t imm) { return (imm) <= 0xfff && (imm) >= -0x1000; } \
-    bool isIMM19(int32_t imm) { return (imm) <= 0x3ffff && (imm) >= -0x40000; } \
-    bool isIMM22(int32_t imm) { return (imm) <= 0x1fffff && (imm) >= -0x200000; } \
-    void JMP_long_nocheck(int32_t t); \
+    static bool isIMM13(int32_t imm) { return (imm) <= 0xfff && (imm) >= -0x1000; } \
+    static bool isIMM19(int32_t imm) { return (imm) <= 0x3ffff && (imm) >= -0x40000; } \
+    static bool isIMM22(int32_t imm) { return (imm) <= 0x1fffff && (imm) >= -0x200000; } \
     void JMP_long(int32_t t); \
     void JMP_long_placeholder(); \
-    int32_t JCC(void *t); \
-    void JMP(void *t); \
+    int32_t JCC(void* t); \
+    void JMP(void* t); \
     void MR(Register rd, Register rs);
 }
 #endif // __nanojit_NativeSparc__
index c8fd987..3caf5cf 100644
--- a/yarr/yarr/RegexCompiler.cpp
+++ b/yarr/yarr/RegexCompiler.cpp
@@ -34,6 +34,12 @@ namespace JSC { namespace Yarr {
 
 #include "RegExpJitTables.h"
 
+#if WTF_CPU_SPARC
+#define BASE_FRAME_SIZE 24
+#else
+#define BASE_FRAME_SIZE 0
+#endif
+
 class CharacterClassConstructor {
 public:
     CharacterClassConstructor(bool isCaseInsensitive = false)
@@ -592,7 +598,7 @@ public:
 
     void setupOffsets()
     {
-        setupDisjunctionOffsets(m_pattern.m_body, 0, 0);
+        setupDisjunctionOffsets(m_pattern.m_body, BASE_FRAME_SIZE, 0);
     }
 
     // This optimization identifies sets of parentheses that we will never need to backtrack.
diff --git a/js/src/yarr/yarr/RegexJIT.cpp b/js/src/yarr/yarr/RegexJIT.cpp
index 9b58ee7..e11907a 100644
--- a/yarr/yarr/RegexJIT.cpp
+++ b/yarr/yarr/RegexJIT.cpp
@@ -62,6 +62,16 @@ class RegexGenerator : private MacroAssembler {
     static const RegisterID regT1 = MIPSRegisters::t5;
 
     static const RegisterID returnRegister = MIPSRegisters::v0;
+#elif WTF_CPU_SPARC
+    static const RegisterID input = SparcRegisters::i0;
+    static const RegisterID index = SparcRegisters::i1;
+    static const RegisterID length = SparcRegisters::i2;
+    static const RegisterID output = SparcRegisters::i3;
+
+    static const RegisterID regT0 = SparcRegisters::i4;
+    static const RegisterID regT1 = SparcRegisters::i5;
+
+    static const RegisterID returnRegister = SparcRegisters::i0;
 #elif WTF_CPU_X86
     static const RegisterID input = X86Registers::eax;
     static const RegisterID index = X86Registers::edx;
@@ -564,8 +574,13 @@ class RegexGenerator : private MacroAssembler {
     void generatePatternCharacterPair(TermGenerationState& state)
     {
         const RegisterID character = regT0;
+#if WTF_CPU_BIG_ENDIAN
+        UChar ch2 = state.term().patternCharacter;
+        UChar ch1 = state.lookaheadTerm().patternCharacter;
+#else
         UChar ch1 = state.term().patternCharacter;
         UChar ch2 = state.lookaheadTerm().patternCharacter;
+#endif
 
         int mask = 0;
         int chPair = ch1 | (ch2 << 16);
@@ -1449,6 +1464,10 @@ class RegexGenerator : private MacroAssembler {
         push(ARMRegisters::r8); // scratch register
 #endif
         move(ARMRegisters::r3, output);
+#elif WTF_CPU_SPARC
+        save(Imm32(-m_pattern.m_body->m_callFrameSize * sizeof(void*)));
+        // set m_callFrameSize to 0 avoid and stack movement later.
+        m_pattern.m_body->m_callFrameSize = 0;
 #elif WTF_CPU_MIPS
         // Do nothing.
 #endif
@@ -1471,6 +1490,9 @@ class RegexGenerator : private MacroAssembler {
         pop(ARMRegisters::r6);
         pop(ARMRegisters::r5);
         pop(ARMRegisters::r4);
+#elif WTF_CPU_SPARC
+        ret_and_restore();
+        return;
 #elif WTF_CPU_MIPS
         // Do nothing
 #endif
